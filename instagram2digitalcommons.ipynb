{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1aiXrNx7UvpSmXmBZ7g20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e3la/instagram2digitalcommons/blob/main/instagram2digitalcommons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first block of code asks if you want to upload your zip or if you want to grab it off google drive. While developing this tool I kept having google colab forget about my zip file and I realized I couldn't keep spending so much time waiting for it to re-upload while I developed it, and thus, this choice was born."
      ],
      "metadata": {
        "id": "lJz0rlIMnftE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxMQIIdgkj6x",
        "outputId": "507b9bdf-1ea1-4f1a-a3cb-2b95d2439bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "How do you want to provide the ZIP file?\n",
            "1. Upload directly to Colab (for smaller files, keeps original name).\n",
            "2. Use the ZIP file from Google Drive (searches 'MyDrive/i2dc/' for a unique .zip file).\n",
            "Enter choice (1 or 2): 2\n",
            "--------------------------------------------------\n",
            "Selected: Use ZIP file from Google Drive.\n",
            "Attempting to mount Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive\n",
            "Searching for a unique .zip file in: '/content/drive/MyDrive/i2dc'\n",
            "Found ZIP file: 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'\n",
            "File path in Colab (via Drive): /content/drive/MyDrive/i2dc/instagram-umsllibraries-2025-03-07-Ardjbhx1.zip\n",
            "--------------------------------------------------\n",
            "\n",
            "Proceeding with ZIP file: 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'\n",
            "Located at: /content/drive/MyDrive/i2dc/instagram-umsllibraries-2025-03-07-Ardjbhx1.zip\n",
            "Extraction target directory: '/content/extracted_data'\n",
            "Attempting to extract 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'...\n",
            "Successfully extracted 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip' to '/content/extracted_data'\n",
            "\n",
            "Contents of the extracted folder:\n",
            "- logged_information\n",
            "- security_and_login_information\n",
            "- media\n",
            "- apps_and_websites_off_of_instagram\n",
            "- preferences\n",
            "- personal_information\n",
            "- connections\n",
            "- ads_information\n",
            "- your_instagram_activity\n",
            "--------------------------------------------------\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil # For file operations like deleting directories\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "\n",
        "zip_filepath = None # This will store the path to the zip file\n",
        "uploaded_filename_original = None # To store the original name from upload\n",
        "\n",
        "# --- Helper function to clean up previous uploads from /content/ if method 1 is chosen ---\n",
        "def cleanup_content_directory(filename_to_keep=None):\n",
        "    \"\"\"Removes all files and folders from /content/ except specified ones.\"\"\"\n",
        "    print(\"Cleaning up /content/ directory...\")\n",
        "    items_to_preserve = [\"drive\", \"sample_data\"] # Default Colab folders\n",
        "    if filename_to_keep:\n",
        "        items_to_preserve.append(os.path.basename(filename_to_keep))\n",
        "\n",
        "    for item in os.listdir(\"/content/\"):\n",
        "        if item in items_to_preserve:\n",
        "            continue\n",
        "        item_path = os.path.join(\"/content/\", item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)\n",
        "                # print(f\"Removed file: {item_path}\")\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "                # print(f\"Removed directory: {item_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {item_path}. Reason: {e}\")\n",
        "    print(\"Cleanup of /content/ complete.\")\n",
        "\n",
        "# --- Ask the user how they want to provide the file ---\n",
        "while True:\n",
        "    print(\"-\" * 50)\n",
        "    method = input(\n",
        "        \"How do you want to provide the ZIP file?\\n\"\n",
        "        \"1. Upload directly to Colab (for smaller files, keeps original name).\\n\"\n",
        "        \"2. Use the ZIP file from Google Drive (searches 'MyDrive/i2dc/' for a unique .zip file).\\n\"\n",
        "        \"Enter choice (1 or 2): \"\n",
        "    ).strip()\n",
        "    if method in ['1', '2']:\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Option 1: Direct Upload ---\n",
        "if method == '1':\n",
        "    print(\"Selected: Upload directly to Colab.\")\n",
        "    print(\"Please wait for the upload dialog and select your ZIP file...\")\n",
        "\n",
        "    # Clean up /content/ before new upload\n",
        "    cleanup_content_directory()\n",
        "\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"No file was uploaded. Exiting.\")\n",
        "        else:\n",
        "            # Get the uploaded file name (key in the 'uploaded' dict)\n",
        "            uploaded_filename_original = list(uploaded.keys())[0]\n",
        "\n",
        "            if not uploaded_filename_original.lower().endswith('.zip'):\n",
        "                print(f\"Error: The uploaded file '{uploaded_filename_original}' is not a ZIP file.\")\n",
        "                # Clean up the wrongly uploaded file\n",
        "                wrong_file_path = os.path.join(\"/content/\", uploaded_filename_original)\n",
        "                if os.path.exists(wrong_file_path):\n",
        "                    os.remove(wrong_file_path)\n",
        "                uploaded_filename_original = None # Reset as it's not a valid zip\n",
        "            else:\n",
        "                # The file is uploaded directly to /content/\n",
        "                zip_filepath = os.path.join(\"/content/\", uploaded_filename_original)\n",
        "                print(f\"Successfully uploaded: '{uploaded_filename_original}'\")\n",
        "                print(f\"File path in Colab: {zip_filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during upload: {e}\")\n",
        "        print(\"If the file is too large, please try the Google Drive option next time.\")\n",
        "\n",
        "# --- Option 2: Google Drive ---\n",
        "elif method == '2':\n",
        "    print(\"Selected: Use ZIP file from Google Drive.\")\n",
        "    print(\"Attempting to mount Google Drive...\")\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully at /content/drive\")\n",
        "\n",
        "        gdrive_my_drive_path = \"/content/drive/MyDrive/\"\n",
        "        target_folder_name = \"i2dc\"\n",
        "        target_folder_path_in_drive = os.path.join(gdrive_my_drive_path, target_folder_name)\n",
        "\n",
        "        print(f\"Searching for a unique .zip file in: '{target_folder_path_in_drive}'\")\n",
        "\n",
        "        if not os.path.isdir(target_folder_path_in_drive):\n",
        "            print(f\"Error: The folder '{target_folder_path_in_drive}' ('{target_folder_name}' in your MyDrive) does not exist.\")\n",
        "            print(f\"Please ensure you have a folder named '{target_folder_name}' directly under 'My Drive' containing your .zip file.\")\n",
        "        else:\n",
        "            zip_files_found = []\n",
        "            for item_name in os.listdir(target_folder_path_in_drive):\n",
        "                item_full_path = os.path.join(target_folder_path_in_drive, item_name)\n",
        "                if os.path.isfile(item_full_path) and item_name.lower().endswith('.zip'):\n",
        "                    zip_files_found.append(item_full_path)\n",
        "\n",
        "            if len(zip_files_found) == 0:\n",
        "                print(f\"No .zip files found in '{target_folder_path_in_drive}'.\")\n",
        "            elif len(zip_files_found) == 1:\n",
        "                zip_filepath = zip_files_found[0]\n",
        "                uploaded_filename_original = os.path.basename(zip_filepath) # Get original name from path\n",
        "                print(f\"Found ZIP file: '{uploaded_filename_original}'\")\n",
        "                print(f\"File path in Colab (via Drive): {zip_filepath}\")\n",
        "            else:\n",
        "                print(f\"Error: Multiple .zip files found in '{target_folder_path_in_drive}':\")\n",
        "                for f_path in zip_files_found:\n",
        "                    print(f\" - {os.path.basename(f_path)}\")\n",
        "                print(\"Please ensure there is only one .zip file in that folder for this option to work automatically.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while mounting or accessing Google Drive: {e}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Proceed with the zip file if one was successfully identified ---\n",
        "if zip_filepath and os.path.exists(zip_filepath):\n",
        "    print(f\"\\nProceeding with ZIP file: '{uploaded_filename_original}'\")\n",
        "    print(f\"Located at: {zip_filepath}\")\n",
        "\n",
        "    # --- Your next steps using zip_filepath ---\n",
        "    extract_to_folder = \"/content/extracted_data\" # Define your extraction path\n",
        "\n",
        "    # Clean up previous extraction if it exists\n",
        "    if os.path.exists(extract_to_folder):\n",
        "        print(f\"Cleaning up previous extraction at '{extract_to_folder}'...\")\n",
        "        shutil.rmtree(extract_to_folder)\n",
        "    os.makedirs(extract_to_folder, exist_ok=True)\n",
        "    print(f\"Extraction target directory: '{extract_to_folder}'\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Attempting to extract '{uploaded_filename_original}'...\")\n",
        "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_folder)\n",
        "        print(f\"Successfully extracted '{uploaded_filename_original}' to '{extract_to_folder}'\")\n",
        "\n",
        "        print(\"\\nContents of the extracted folder:\")\n",
        "        extracted_items = os.listdir(extract_to_folder)\n",
        "        if not extracted_items:\n",
        "            print(\"(The extracted folder is empty)\")\n",
        "        else:\n",
        "            for item in extracted_items:\n",
        "                print(f\"- {item}\")\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: The file '{uploaded_filename_original}' is not a valid ZIP file or is corrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "elif method == '1' and not zip_filepath and uploaded_filename_original is None and 'uploaded' in locals() and not uploaded:\n",
        "    # This case is already handled by \"No file was uploaded.\"\n",
        "    pass\n",
        "elif method == '1' and not zip_filepath and uploaded_filename_original is None:\n",
        "    # This case is for \"uploaded file was not a ZIP\"\n",
        "    print(\"\\nCannot proceed as the uploaded file was not a valid ZIP file.\")\n",
        "elif not zip_filepath:\n",
        "    print(\"\\nNo ZIP file was successfully specified or found. Cannot proceed with further operations.\")\n",
        "elif not os.path.exists(zip_filepath): # Should be rare if logic above is correct\n",
        "     print(f\"\\nError: The determined ZIP file path '{zip_filepath}' does not seem to exist. This is unexpected. Cannot proceed.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Script finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Revised: Inspect contents of the Instagram archive using your file counting logic\n",
        "import os\n",
        "\n",
        "MEDIA_DIR = \"/content/extracted_data\"\n",
        "\n",
        "print(f\"\\nüîé Scanning '{MEDIA_DIR}' for Instagram data...\")\n",
        "\n",
        "file_count = 0\n",
        "image_count = 0\n",
        "video_count = 0\n",
        "\n",
        "for root, dirs, files in os.walk(MEDIA_DIR):\n",
        "    for file in files:\n",
        "        file_count += 1\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):\n",
        "            image_count += 1\n",
        "        elif file.lower().endswith('.mp4'):\n",
        "            video_count += 1\n",
        "\n",
        "print(\"\\nüìä Archive Summary\")\n",
        "print(\"-----------------------------------------\")\n",
        "print(f\"Total files: {file_count}\")\n",
        "print(f\"Image files: {image_count}\")\n",
        "print(f\"MP4 video files: {video_count}\")\n",
        "print(\"You can also browse these files using the 'Files' panel on the left sidebar.\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "# üì¶ Count Instagram media types from extracted JSON data\n",
        "import json\n",
        "\n",
        "# Base directory: previously defined as extract_dir\n",
        "base_dir = MEDIA_DIR\n",
        "activity_dir = None\n",
        "\n",
        "# Base directory: previously defined as extract_dir\n",
        "base_dir = MEDIA_DIR\n",
        "\n",
        "# Default activity folder name\n",
        "activity_dir = 'your_instagram_activity'\n",
        "print(f\"üìÅ Using default Instagram activity folder: '{activity_dir}'\")\n",
        "\n",
        "media_json_path = os.path.join(base_dir, activity_dir, 'media')\n",
        "print(f\"\\nüìÇ Searching media JSON files in: {media_json_path}\")\n",
        "# Define target files\n",
        "json_files = {\n",
        "    \"posts\": \"posts_1.json\",\n",
        "    \"reels\": \"reels.json\",\n",
        "    \"stories\": \"stories.json\"\n",
        "}\n",
        "\n",
        "counts = {}\n",
        "\n",
        "for media_type, filename in json_files.items():\n",
        "    file_path = os.path.join(media_json_path, filename)\n",
        "    print(f\"\\nüîç {media_type.capitalize()}: {filename}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(\"   ‚ö†Ô∏è File not found.\")\n",
        "            counts[media_type] = 0\n",
        "            continue\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if isinstance(data, list):  # posts_1.json\n",
        "            counts[media_type] = len(data)\n",
        "            print(f\"   ‚úÖ Found {counts[media_type]} entries (flat list).\")\n",
        "\n",
        "        elif isinstance(data, dict):\n",
        "            top_key = {\n",
        "                \"reels\": \"ig_reels_media\",\n",
        "                \"stories\": \"ig_stories\"\n",
        "            }.get(media_type)\n",
        "\n",
        "            if top_key not in data:\n",
        "                print(f\"   ‚ö†Ô∏è Key '{top_key}' not found.\")\n",
        "                counts[media_type] = 0\n",
        "                continue\n",
        "\n",
        "            flat_list = []\n",
        "            for group in data[top_key]:\n",
        "                if isinstance(group, dict) and \"media\" in group:\n",
        "                    flat_list.extend(group[\"media\"])\n",
        "                else:\n",
        "                    flat_list.append(group)\n",
        "\n",
        "            counts[media_type] = len(flat_list)\n",
        "            print(f\"   ‚úÖ Found {counts[media_type]} entries (flattened).\")\n",
        "\n",
        "        else:\n",
        "            print(\"   ‚ùå Unrecognized JSON structure.\")\n",
        "            counts[media_type] = 0\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"   ‚ùå Error decoding JSON.\")\n",
        "        counts[media_type] = 0\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Unexpected error: {e}\")\n",
        "        counts[media_type] = 0\n",
        "\n",
        "# prompt: this is in the json\n",
        "# {\n",
        "#   \"profile_user\": [\n",
        "#     {\n",
        "...\n",
        "#         \"Username\": {\n",
        "#           \"href\": \"\",\n",
        "#           \"value\": \"umsllibraries\",\n",
        "#           \"timestamp\": 0\n",
        "# in the file /content/extracted_data/personal_information/personal_information/personal_information.json\n",
        "# and I want to pull out just the Username value\n",
        "\n",
        "# Path to the JSON file\n",
        "json_file_path = \"/content/extracted_data/personal_information/personal_information/personal_information.json\"\n",
        "\n",
        "# Load the JSON data\n",
        "try:\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Navigate through the structure to find the Username value\n",
        "    username_value = None\n",
        "    if isinstance(data, dict) and \"profile_user\" in data and isinstance(data[\"profile_user\"], list) and len(data[\"profile_user\"]) > 0:\n",
        "        # Assuming the relevant data is in the first element of the profile_user list\n",
        "        profile_info = data[\"profile_user\"][0]\n",
        "        if isinstance(profile_info, dict) and \"string_map_data\" in profile_info and isinstance(profile_info[\"string_map_data\"], dict):\n",
        "            string_data = profile_info[\"string_map_data\"]\n",
        "            if \"Username\" in string_data and isinstance(string_data[\"Username\"], dict) and \"value\" in string_data[\"Username\"]:\n",
        "                username_value = string_data[\"Username\"][\"value\"]\n",
        "\n",
        "    # Print the extracted username\n",
        "    if username_value is not None:\n",
        "        print(f\"\\nExtracted Username: {username_value}\")\n",
        "    else:\n",
        "        print(\"Username not found in the expected structure.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {json_file_path}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from {json_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjEQbVszpZ6s",
        "outputId": "346e34c2-953f-469c-d41f-15836676b141"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Scanning '/content/extracted_data' for Instagram data...\n",
            "\n",
            "üìä Archive Summary\n",
            "-----------------------------------------\n",
            "Total files: 1549\n",
            "Image files: 1229\n",
            "MP4 video files: 178\n",
            "You can also browse these files using the 'Files' panel on the left sidebar.\n",
            "-----------------------------------------\n",
            "üìÅ Using default Instagram activity folder: 'your_instagram_activity'\n",
            "\n",
            "üìÇ Searching media JSON files in: /content/extracted_data/your_instagram_activity/media\n",
            "\n",
            "üîç Posts: posts_1.json\n",
            "   ‚úÖ Found 646 entries (flat list).\n",
            "\n",
            "üîç Reels: reels.json\n",
            "   ‚úÖ Found 25 entries (flattened).\n",
            "\n",
            "üîç Stories: stories.json\n",
            "   ‚úÖ Found 485 entries (flattened).\n",
            "\n",
            "Extracted Username: umsllibraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO RUN BEFORE CONTENT PROCESSING CELLS\n",
        "# This cell will iterate through all .json files in the media directory,\n",
        "# back them up, and attempt to fix mojibake in known text fields using ftfy.\n",
        "\n",
        "# 1. Install ftfy\n",
        "!pip install ftfy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil # For copying files\n",
        "import ftfy\n",
        "\n",
        "print(\"--- Starting JSON fixing process for all media files ---\")\n",
        "\n",
        "# Define the path to the media directory\n",
        "MEDIA_DIR_BASE = \"/content/extracted_data\" # Make sure this matches your setup\n",
        "TARGET_MEDIA_DIR = os.path.join(MEDIA_DIR_BASE, 'your_instagram_activity', 'media')\n",
        "\n",
        "if not os.path.isdir(TARGET_MEDIA_DIR):\n",
        "    print(f\"‚ùå ERROR: Media directory not found at: {TARGET_MEDIA_DIR}\")\n",
        "    print(\"Please ensure the extraction in the first cell was successful and the path is correct.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found media directory: {TARGET_MEDIA_DIR}\")\n",
        "\n",
        "    total_files_processed = 0\n",
        "    total_files_changed_by_ftfy = 0\n",
        "    total_titles_fixed_overall = 0\n",
        "\n",
        "    # Helper function to fix text fields in common Instagram JSON structures\n",
        "    def fix_text_fields_in_data(loaded_data, filename_basename):\n",
        "        \"\"\"\n",
        "        Modifies loaded_data in-place by applying ftfy.fix_text to known text fields.\n",
        "        Returns: (bool: data_was_changed, int: titles_fixed_count)\n",
        "        \"\"\"\n",
        "        data_was_changed_locally = False\n",
        "        titles_fixed_count_locally = 0\n",
        "\n",
        "        def _apply_fix(obj, key):\n",
        "            nonlocal data_was_changed_locally, titles_fixed_count_locally\n",
        "            original_text = obj.get(key)\n",
        "            if isinstance(original_text, str) and original_text: # Only fix non-empty strings\n",
        "                fixed_text = ftfy.fix_text(original_text)\n",
        "                if fixed_text != original_text:\n",
        "                    obj[key] = fixed_text\n",
        "                    data_was_changed_locally = True\n",
        "                    titles_fixed_count_locally += 1\n",
        "\n",
        "        if filename_basename == \"reels.json\" or filename_basename == \"stories.json\":\n",
        "            top_level_key = \"ig_reels_media\" if filename_basename == \"reels.json\" else \"ig_stories\"\n",
        "            if isinstance(loaded_data, dict) and top_level_key in loaded_data:\n",
        "                item_list = loaded_data.get(top_level_key, [])\n",
        "                for item_dict in item_list: # Can be group or direct media item\n",
        "                    if isinstance(item_dict, dict):\n",
        "                        if \"media\" in item_dict and isinstance(item_dict[\"media\"], list): # It's a group\n",
        "                            for media_sub_item_dict in item_dict[\"media\"]:\n",
        "                                if isinstance(media_sub_item_dict, dict):\n",
        "                                    _apply_fix(media_sub_item_dict, \"title\")\n",
        "                        else: # It's a direct media item in the list\n",
        "                            _apply_fix(item_dict, \"title\")\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è Structure of {filename_basename} not as expected (missing '{top_level_key}' or not a dict). Skipping detailed fix.\")\n",
        "\n",
        "\n",
        "        elif filename_basename == \"posts_1.json\":\n",
        "            if isinstance(loaded_data, list): # posts_1.json is a list of post objects\n",
        "                for post_item_dict in loaded_data:\n",
        "                    if isinstance(post_item_dict, dict):\n",
        "                        _apply_fix(post_item_dict, \"title\") # Overall post caption/title\n",
        "\n",
        "                        media_list_in_post = post_item_dict.get(\"media\", [])\n",
        "                        if isinstance(media_list_in_post, list):\n",
        "                            for media_item_dict in media_list_in_post:\n",
        "                                if isinstance(media_item_dict, dict):\n",
        "                                    _apply_fix(media_item_dict, \"title\") # Caption for individual media\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è Structure of {filename_basename} not as expected (not a list). Skipping detailed fix.\")\n",
        "\n",
        "        # Add handlers for other specific JSON files here if needed\n",
        "        # else:\n",
        "        #     print(f\"    ‚ÑπÔ∏è No specific ftfy fixing logic defined for {filename_basename}. Text fields might not be fixed.\")\n",
        "\n",
        "        return data_was_changed_locally, titles_fixed_count_locally\n",
        "\n",
        "    # Iterate through all files in the target directory\n",
        "    for filename in os.listdir(TARGET_MEDIA_DIR):\n",
        "        if filename.lower().endswith('.json'):\n",
        "            total_files_processed += 1\n",
        "            current_json_path = os.path.join(TARGET_MEDIA_DIR, filename)\n",
        "            print(f\"\\n--- Processing: {filename} ---\")\n",
        "\n",
        "            # 2. Backup original file\n",
        "            backup_json_path = os.path.join(TARGET_MEDIA_DIR, f\"{os.path.splitext(filename)[0]}-original.json\")\n",
        "            if not os.path.exists(backup_json_path):\n",
        "                try:\n",
        "                    shutil.copy2(current_json_path, backup_json_path)\n",
        "                    print(f\"  üíæ Original backed up to: {os.path.basename(backup_json_path)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è WARNING: Could not create backup of {filename}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ‚ÑπÔ∏è Backup '{os.path.basename(backup_json_path)}' already exists. Not overwriting.\")\n",
        "\n",
        "            # 3. Load, Fix, and Save the current JSON file\n",
        "            try:\n",
        "                print(f\"  L_oading {filename} for fixing...\")\n",
        "                with open(current_json_path, 'r', encoding='utf-8') as f:\n",
        "                    json_content = json.load(f)\n",
        "                print(f\"    {filename} loaded successfully.\")\n",
        "\n",
        "                # Apply ftfy fixes based on file type\n",
        "                data_actually_changed_in_file, titles_fixed_in_file = fix_text_fields_in_data(json_content, filename)\n",
        "\n",
        "                if titles_fixed_in_file > 0:\n",
        "                    print(f\"    üîß Titles/text fields fixed by ftfy in {filename}: {titles_fixed_in_file}\")\n",
        "                    total_titles_fixed_overall += titles_fixed_in_file\n",
        "\n",
        "                if data_actually_changed_in_file:\n",
        "                    total_files_changed_by_ftfy +=1\n",
        "                    print(f\"  S_aving modified {filename} back to disk...\")\n",
        "                    try:\n",
        "                        with open(current_json_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump(json_content, f, ensure_ascii=False, indent=2)\n",
        "                        print(f\"    ‚úÖ Successfully saved fixed {filename}.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ùå ERROR saving modified {filename}: {e}\")\n",
        "                elif titles_fixed_in_file > 0 and not data_actually_changed_in_file : # Should not happen if titles_fixed_in_file > 0\n",
        "                    print(f\"    ‚ÑπÔ∏è ftfy processed text in {filename} but resulted in no net change. File not rewritten.\")\n",
        "                else: # No titles fixed or no changes made\n",
        "                    print(f\"    ‚ÑπÔ∏è No changes made by ftfy to {filename}. File not rewritten.\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"  ‚ùå ERROR: Could not decode JSON from {filename}. The file might be corrupted. Skipping this file.\")\n",
        "            except FileNotFoundError:\n",
        "                 print(f\"  ‚ùå ERROR: File {filename} not found during processing loop (should not happen if listed). Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå An unexpected error occurred while processing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Summary of JSON fixing process ---\")\n",
        "    print(f\"Total .json files found and processed: {total_files_processed}\")\n",
        "    print(f\"Total files modified by ftfy and resaved: {total_files_changed_by_ftfy}\")\n",
        "    print(f\"Total text fields fixed across all files: {total_titles_fixed_overall}\")\n",
        "    print(\"--- Finished JSON fixing process for all media files ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Q8DZoU9kmu",
        "outputId": "07620b2b-68a1-406f-d85e-a5165dac1caf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting JSON fixing process for all media files ---\n",
            "‚úÖ Found media directory: /content/extracted_data/your_instagram_activity/media\n",
            "\n",
            "--- Processing: posts_1.json ---\n",
            "  ‚ÑπÔ∏è Backup 'posts_1-original.json' already exists. Not overwriting.\n",
            "  L_oading posts_1.json for fixing...\n",
            "    posts_1.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to posts_1.json. File not rewritten.\n",
            "\n",
            "--- Processing: reels-original.json ---\n",
            "  ‚ÑπÔ∏è Backup 'reels-original-original.json' already exists. Not overwriting.\n",
            "  L_oading reels-original.json for fixing...\n",
            "    reels-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to reels-original.json. File not rewritten.\n",
            "\n",
            "--- Processing: stories.json ---\n",
            "  ‚ÑπÔ∏è Backup 'stories-original.json' already exists. Not overwriting.\n",
            "  L_oading stories.json for fixing...\n",
            "    stories.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to stories.json. File not rewritten.\n",
            "\n",
            "--- Processing: reels.json ---\n",
            "  ‚ÑπÔ∏è Backup 'reels-original.json' already exists. Not overwriting.\n",
            "  L_oading reels.json for fixing...\n",
            "    reels.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to reels.json. File not rewritten.\n",
            "\n",
            "--- Processing: profile_photos-original.json ---\n",
            "  üíæ Original backed up to: profile_photos-original-original.json\n",
            "  L_oading profile_photos-original.json for fixing...\n",
            "    profile_photos-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to profile_photos-original.json. File not rewritten.\n",
            "\n",
            "--- Processing: recently_deleted_content.json ---\n",
            "  ‚ÑπÔ∏è Backup 'recently_deleted_content-original.json' already exists. Not overwriting.\n",
            "  L_oading recently_deleted_content.json for fixing...\n",
            "    recently_deleted_content.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to recently_deleted_content.json. File not rewritten.\n",
            "\n",
            "--- Processing: profile_photos.json ---\n",
            "  ‚ÑπÔ∏è Backup 'profile_photos-original.json' already exists. Not overwriting.\n",
            "  L_oading profile_photos.json for fixing...\n",
            "    profile_photos.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to profile_photos.json. File not rewritten.\n",
            "\n",
            "--- Processing: stories-original.json ---\n",
            "  üíæ Original backed up to: stories-original-original.json\n",
            "  L_oading stories-original.json for fixing...\n",
            "    stories-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to stories-original.json. File not rewritten.\n",
            "\n",
            "--- Processing: recently_deleted_content-original.json ---\n",
            "  üíæ Original backed up to: recently_deleted_content-original-original.json\n",
            "  L_oading recently_deleted_content-original.json for fixing...\n",
            "    recently_deleted_content-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to recently_deleted_content-original.json. File not rewritten.\n",
            "\n",
            "--- Processing: reels-original-original.json ---\n",
            "  üíæ Original backed up to: reels-original-original-original.json\n",
            "  L_oading reels-original-original.json for fixing...\n",
            "    reels-original-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to reels-original-original.json. File not rewritten.\n",
            "\n",
            "--- Processing: posts_1-original.json ---\n",
            "  üíæ Original backed up to: posts_1-original-original.json\n",
            "  L_oading posts_1-original.json for fixing...\n",
            "    posts_1-original.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to posts_1-original.json. File not rewritten.\n",
            "\n",
            "--- Summary of JSON fixing process ---\n",
            "Total .json files found and processed: 11\n",
            "Total files modified by ftfy and resaved: 0\n",
            "Total text fields fixed across all files: 0\n",
            "--- Finished JSON fixing process for all media files ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from shutil import copy2\n",
        "import re # For hashtag extraction\n",
        "\n",
        "# Ensure MEDIA_DIR is defined, as it's used in subsequent cells\n",
        "MEDIA_DIR = \"/content/extracted_data\" # This should be consistent with the extraction path\n",
        "\n",
        "media_type = 'reels' # We are focusing on reels\n",
        "output_dir = os.path.join(MEDIA_DIR, f'{media_type}_export_dc_format') # New output dir name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Excel and ZIP file paths\n",
        "excel_filename = f'{media_type}_metadata_dc_format.xlsx'\n",
        "excel_path = os.path.join(output_dir, excel_filename)\n",
        "readme_path = os.path.join(output_dir, 'README.txt')\n",
        "zip_path = os.path.join(output_dir, f'{media_type}_package_dc_format.zip') # New zip name\n",
        "\n",
        "# Attempt to get username_value from global scope (set in a previous cell)\n",
        "instagram_handle = globals().get('username_value', \"unknown_user\")\n",
        "print(f\"Using Instagram handle: {instagram_handle}\")\n",
        "\n",
        "# --- Helper function to extract hashtags ---\n",
        "def extract_hashtags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return \", \".join(hashtags)\n",
        "\n",
        "# --- Load Reels JSON Data ---\n",
        "instagram_media_data = {}\n",
        "try:\n",
        "    reels_json_path = os.path.join(MEDIA_DIR, 'your_instagram_activity', 'media', 'reels.json')\n",
        "    if os.path.exists(reels_json_path):\n",
        "         with open(reels_json_path, 'r', encoding='utf-8') as f:\n",
        "            reels_full_data = json.load(f)\n",
        "            if isinstance(reels_full_data, dict) and \"ig_reels_media\" in reels_full_data:\n",
        "                 flat_list = []\n",
        "                 for group in reels_full_data[\"ig_reels_media\"]:\n",
        "                     if isinstance(group, dict) and \"media\" in group:\n",
        "                         flat_list.extend(group[\"media\"])\n",
        "                     else:\n",
        "                         flat_list.append(group)\n",
        "                 instagram_media_data['reels'] = flat_list # Assign to 'reels' key\n",
        "            else:\n",
        "                print(\"Warning: Reels JSON structure unexpected or 'ig_reels_media' key missing.\")\n",
        "                instagram_media_data['reels'] = []\n",
        "    else:\n",
        "        print(f\"Reels JSON file not found at: {reels_json_path}\")\n",
        "        instagram_media_data['reels'] = []\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Reels JSON for export: {e}\")\n",
        "    instagram_media_data['reels'] = []\n",
        "\n",
        "# --- Process Reels Data ---\n",
        "reels_data_to_process = instagram_media_data.get('reels') # Get data from 'reels' key\n",
        "\n",
        "if not reels_data_to_process:\n",
        "    print(\"No Reels data loaded or found for export.\")\n",
        "else:\n",
        "    print(f\"Preparing to export metadata for {len(reels_data_to_process)} Reels in Digital Commons format...\")\n",
        "\n",
        "    excel_data_rows = []\n",
        "\n",
        "    # Define the new column order for the Excel sheet\n",
        "    column_names = [\n",
        "        'title', 'fulltext_url', 'additional_files', 'keywords', 'abstract',\n",
        "        'author1_fname', 'author1_mname', 'author1_lname', 'author1_suffix',\n",
        "        'author1_email', 'author1_institution', 'author1_is_corporate',\n",
        "        'author2_fname', 'author2_mname', 'author2_lname', 'author2_suffix',\n",
        "        'author2_email', 'author2_institution', 'author2_is_corporate',\n",
        "        'author3_fname', 'author3_mname', 'author3_lname', 'author3_suffix',\n",
        "        'author3_email', 'author3_institution', 'author3_is_corporate',\n",
        "        'author4_fname', 'author4_mname', 'author4_lname', 'author4_suffix',\n",
        "        'author4_email', 'author4_institution', 'author4_is_corporate',\n",
        "        'disciplines', 'instagram_username', 'document_type'\n",
        "    ]\n",
        "\n",
        "    copied_files_for_zip = []\n",
        "\n",
        "    for i, item in enumerate(reels_data_to_process):\n",
        "        original_reel_uri = item.get('uri')\n",
        "        if not original_reel_uri:\n",
        "            print(f\"Warning: Reel item {i} has no URI, skipping.\")\n",
        "            continue\n",
        "\n",
        "        media_path = os.path.join(MEDIA_DIR, original_reel_uri)\n",
        "        if not os.path.exists(media_path):\n",
        "            print(f\"Warning: Media file not found - {media_path}, skipping reel item.\")\n",
        "            continue\n",
        "\n",
        "        # --- Date and Filename (Reel Video) ---\n",
        "        timestamp = item.get('creation_timestamp')\n",
        "        date_obj = datetime.fromtimestamp(timestamp) if timestamp else datetime.now()\n",
        "        date_str_for_title = date_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "        ext = os.path.splitext(original_reel_uri)[-1]\n",
        "        handle_for_filename = instagram_handle if instagram_handle else \"unknown_user\"\n",
        "        original_file_basename = os.path.splitext(os.path.basename(original_reel_uri))[0]\n",
        "        sanitized_original_basename = ''.join(c if c.isalnum() else '_' for c in original_file_basename)\n",
        "\n",
        "        # This is the reel video filename, goes into 'fulltext_url'\n",
        "        reel_export_filename = f\"instagram_{handle_for_filename}_reel_{date_str_for_title}_{i+1}_{sanitized_original_basename}{ext}\"\n",
        "        reel_export_path = os.path.join(output_dir, reel_export_filename)\n",
        "\n",
        "        try:\n",
        "            copy2(media_path, reel_export_path)\n",
        "            copied_files_for_zip.append(reel_export_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying media file {media_path} to {reel_export_path}: {e}. Skipping this reel.\")\n",
        "            continue\n",
        "\n",
        "        # --- Abstract and Keywords ---\n",
        "        original_caption = item.get('title', '') # This is the Instagram caption\n",
        "        keywords_str = extract_hashtags(original_caption)\n",
        "\n",
        "        # --- SRT File (Additional File) ---\n",
        "        srt_export_filename = '' # Initialize to empty\n",
        "        original_srt_uri = ''\n",
        "\n",
        "        try:\n",
        "            subtitles_data = item.get('media_metadata', {}).get('video_metadata', {}).get('subtitles', {})\n",
        "            original_srt_uri = subtitles_data.get('uri', '')\n",
        "\n",
        "            if original_srt_uri:\n",
        "                srt_original_full_path = os.path.join(MEDIA_DIR, original_srt_uri)\n",
        "                if os.path.exists(srt_original_full_path):\n",
        "                    base_export_videoname, _ = os.path.splitext(reel_export_filename)\n",
        "                    srt_export_filename = base_export_videoname + \".srt\" # This goes into 'additional_files'\n",
        "                    srt_export_path = os.path.join(output_dir, srt_export_filename)\n",
        "\n",
        "                    copy2(srt_original_full_path, srt_export_path)\n",
        "                    copied_files_for_zip.append(srt_export_path)\n",
        "                else:\n",
        "                    # srt_export_filename remains empty if source SRT not found\n",
        "                    pass\n",
        "        except (KeyError, TypeError):\n",
        "            # srt_export_filename remains empty\n",
        "            pass\n",
        "\n",
        "        # --- Construct new title ---\n",
        "        dc_title = f\"Instagram {handle_for_filename} {date_str_for_title}\"\n",
        "\n",
        "        # --- Prepare row data for Excel ---\n",
        "        row = {\n",
        "            'title': dc_title,\n",
        "            'fulltext_url': reel_export_filename,\n",
        "            'additional_files': srt_export_filename, # Will be empty if no SRT\n",
        "            'keywords': keywords_str,\n",
        "            'abstract': original_caption,\n",
        "            'author1_fname': '', 'author1_mname': '', 'author1_lname': '', 'author1_suffix': '',\n",
        "            'author1_email': '', 'author1_institution': '', 'author1_is_corporate': '',\n",
        "            'author2_fname': '', 'author2_mname': '', 'author2_lname': '', 'author2_suffix': '',\n",
        "            'author2_email': '', 'author2_institution': '', 'author2_is_corporate': '',\n",
        "            'author3_fname': '', 'author3_mname': '', 'author3_lname': '', 'author3_suffix': '',\n",
        "            'author3_email': '', 'author3_institution': '', 'author3_is_corporate': '',\n",
        "            'author4_fname': '', 'author4_mname': '', 'author4_lname': '', 'author4_suffix': '',\n",
        "            'author4_email': '', 'author4_institution': '', 'author4_is_corporate': '',\n",
        "            'disciplines': '',\n",
        "            'instagram_username': instagram_handle,\n",
        "            'document_type': 'Instagram Reel'\n",
        "        }\n",
        "        excel_data_rows.append(row)\n",
        "\n",
        "    # --- Create DataFrame and save to Excel ---\n",
        "    if excel_data_rows:\n",
        "        df = pd.DataFrame(excel_data_rows, columns=column_names)\n",
        "        try:\n",
        "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"Metadata written to Excel: {excel_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Excel file {excel_path}: {e}\")\n",
        "            print(\"Make sure 'openpyxl' library is installed (pip install openpyxl).\")\n",
        "    else:\n",
        "        print(\"No data to write to Excel.\")\n",
        "\n",
        "    # --- Write README file (Updated) ---\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"\"\"Instagram Reels Export Package (Digital Commons Format)\n",
        "=====================================================\n",
        "\n",
        "Handle: @{instagram_handle}\n",
        "Exported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "This package contains:\n",
        "- Exported Reel video files.\n",
        "- Associated .srt subtitle files (if they existed in the archive and were found), renamed to match their corresponding video files.\n",
        "- An Excel file ({excel_filename}) with metadata for each reel, formatted for potential Digital Commons import.\n",
        "- This README.txt file.\n",
        "\n",
        "Fields in the Excel file ({excel_filename}):\n",
        "- title: A generated title in the format \"Instagram [username] [YYYY-MM-DD]\".\n",
        "- fulltext_url: The filename of the exported reel video file in this package. This file should be uploaded as the primary file.\n",
        "- additional_files: The filename of the exported .srt subtitle file (if available) in this package. This should be uploaded as an additional file.\n",
        "- keywords: Comma-separated hashtags extracted from the reel's original caption.\n",
        "- abstract: The original caption/text of the Instagram reel.\n",
        "- author1_fname to author4_is_corporate: Fields for author information (currently left blank).\n",
        "- disciplines: Field for academic disciplines (currently left blank).\n",
        "- instagram_username: The Instagram handle from which the reel originated.\n",
        "- document_type: Set to \"Instagram Reel\".\n",
        "\n",
        "Generated by Instagram archive processing script.\n",
        "\"\"\")\n",
        "    print(f\"README file written to: {readme_path}\")\n",
        "\n",
        "    # --- Zip everything up ---\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        if os.path.exists(excel_path):\n",
        "            zipf.write(excel_path, arcname=os.path.basename(excel_path))\n",
        "        zipf.write(readme_path, arcname='README.txt')\n",
        "        for file_to_zip in copied_files_for_zip: # Includes videos and SRTs\n",
        "            if os.path.exists(file_to_zip):\n",
        "                zipf.write(file_to_zip, arcname=os.path.basename(file_to_zip))\n",
        "            else:\n",
        "                print(f\"Warning: File {file_to_zip} not found for zipping.\")\n",
        "    print(f\"ZIP archive created: {zip_path}\")\n",
        "\n",
        "# The last cell (for downloading) will need to be updated to point to this new zip_path\n",
        "# e.g., source_zip_path = \"/content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\"\n",
        "# and target_zip_filename = \"reels_dc_format.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS-P7cYjp5xh",
        "outputId": "3993abae-e100-4b62-d094-75ab1aa5144d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Instagram handle: umsllibraries\n",
            "Preparing to export metadata for 25 Reels in Digital Commons format...\n",
            "Metadata written to Excel: /content/extracted_data/reels_export_dc_format/reels_metadata_dc_format.xlsx\n",
            "README file written to: /content/extracted_data/reels_export_dc_format/README.txt\n",
            "ZIP archive created: /content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files # Make sure this is imported\n",
        "\n",
        "# --- Assuming your previous script has run and created the source ZIP ---\n",
        "# Define the source and target paths\n",
        "source_zip_path = \"/content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\"\n",
        "target_base_dir = \"/content/batchup\" # The directory where you want the new ZIP\n",
        "target_zip_filename = \"reels.zip\"     # The desired name for the ZIP in the target directory\n",
        "target_zip_path = os.path.join(target_base_dir, target_zip_filename)\n",
        "\n",
        "# 1. Check if the source ZIP file exists\n",
        "if not os.path.exists(source_zip_path):\n",
        "    print(f\"‚ùå ERROR: Source ZIP file not found at: {source_zip_path}\")\n",
        "    print(\"Please ensure the previous steps to create the ZIP were successful.\")\n",
        "else:\n",
        "    print(f\"‚úîÔ∏è Source ZIP found: {source_zip_path}\")\n",
        "\n",
        "    # 2. Ensure the target directory exists, create it if not\n",
        "    os.makedirs(target_base_dir, exist_ok=True)\n",
        "    print(f\"‚úîÔ∏è Ensured target directory exists: {target_base_dir}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Copy the file\n",
        "        shutil.copy2(source_zip_path, target_zip_path) # copy2 preserves metadata\n",
        "        print(f\"‚úÖ Successfully copied '{os.path.basename(source_zip_path)}' to '{target_zip_path}'\")\n",
        "\n",
        "        # 4. Offer a download link for the new file\n",
        "        print(f\"\\n‚¨áÔ∏è Click the link below to download '{target_zip_filename}':\")\n",
        "        # Note: files.download() directly initiates the download in the browser\n",
        "        # It doesn't print a clickable link in the classic HTML sense in the output cell,\n",
        "        # but Colab's UI will typically show a download prompt or progress.\n",
        "        files.download(target_zip_path)\n",
        "        print(f\"(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\")\n",
        "        print(f\"The file is located at: {target_zip_path} in the Colab environment.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during copy or download: {e}\")\n",
        "\n",
        "# Example of how you might integrate this at the end of your existing script:\n",
        "# ... (your existing script that creates /content/extracted_data/reels_export/reels_package.zip) ...\n",
        "# print(f\"ZIP archive created: {zip_path}\") # zip_path from your previous script would be source_zip_path here\n",
        "\n",
        "# --- Add the copy and download logic here ---\n",
        "# (The code block above would go here, making sure source_zip_path matches\n",
        "# the zip_path variable from your previous script part if you used a variable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "uAKOItDGze-_",
        "outputId": "73efe49b-38e8-4151-e715-f362e63fdbfc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Source ZIP found: /content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\n",
            "‚úîÔ∏è Ensured target directory exists: /content/batchup\n",
            "‚úÖ Successfully copied 'reels_package_dc_format.zip' to '/content/batchup/reels.zip'\n",
            "\n",
            "‚¨áÔ∏è Click the link below to download 'reels.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_222ec22d-99e1-427a-b426-292db662c108\", \"reels.zip\", 154620000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\n",
            "The file is located at: /content/batchup/reels.zip in the Colab environment.\n"
          ]
        }
      ]
    }
  ]
}