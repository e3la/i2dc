{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6tuJNiaHTURuDmsLCP4IM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e3la/i2dc/blob/main/i2dc_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to i2dc an instagram to digitalcommons tool!\n",
        "## This code is the result of vibe coding and was built mostly using gemini.\n",
        "\n",
        "This Colab notebook transforms an Instagram zip archive into structured zip packages for upload to institutional repositories using Digital Commons. You will be guided guided through the process, with a few questions to configure your export before processing your data.\n",
        "\n",
        "A separate ZIP packages for your Posts, Reels, and Stories, will be generated at your request each containing the media files and a corresponding Excel metadata file ready for review and batch upload.\n",
        "\n",
        "First you provide the archive zip you got from instagram, by pressing the play button the left of the box below and follow the steps. When done you can close this cell with the carrot.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFB8cONOKZRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload your zip from instagram\n",
        "#@markdown You can upload directly or connect to google drive and put your zips in MyDrive/i2dc/.\n",
        "\n",
        "from google.colab import files, drive\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import json\n",
        "import re\n",
        "\n",
        "# This variable will be used by the next cells\n",
        "zip_filepath = None\n",
        "\n",
        "def setup_zip_file_interactive():\n",
        "    \"\"\"\n",
        "    Interactively asks the user how they want to provide their Instagram archive ZIP file.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\"\\n--- üìÇ How would you like to provide the Instagram ZIP file? ---\")\n",
        "        method = input(\n",
        "            \"1. Upload from my computer\\n\"\n",
        "            \"2. Use Google Drive\\n\"\n",
        "            \"Enter choice (1 or 2): \"\n",
        "        ).strip()\n",
        "\n",
        "        if method == '1':\n",
        "            print(\"\\nPlease click 'Choose Files' and select your Instagram ZIP archive.\")\n",
        "            uploaded = files.upload()\n",
        "            if uploaded:\n",
        "                filename = list(uploaded.keys())[0]\n",
        "                if filename.lower().endswith('.zip'):\n",
        "                    print(f\"‚úîÔ∏è Successfully uploaded: {filename}\")\n",
        "                    return os.path.join('/content', filename)\n",
        "                else:\n",
        "                    print(f\"‚ùå ERROR: The uploaded file '{filename}' is not a ZIP file. Please try again.\")\n",
        "            else:\n",
        "                print(\"‚ùå No file was uploaded. Please try again.\")\n",
        "\n",
        "        elif method == '2':\n",
        "            print(\"\\nSelected: Use Google Drive.\")\n",
        "            print(\"Connecting to your Google Drive...\")\n",
        "            # 'force_remount=True' ensures a fresh connection every time.\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "            # This is the standard path where the script will look for your files.\n",
        "            gdrive_path = \"/content/drive/MyDrive/i2dc/\"\n",
        "            print(f\"Searching for .zip files in your Google Drive at: '{gdrive_path}'\")\n",
        "\n",
        "            if not os.path.isdir(gdrive_path):\n",
        "                print(f\"‚ùå ERROR: The folder '{gdrive_path}' was not found.\")\n",
        "                print(\"Please create a folder named 'i2dc' in your 'My Drive' and place your ZIP file inside it, then run this cell again.\")\n",
        "                return None\n",
        "\n",
        "            zip_files_found = [os.path.join(gdrive_path, f) for f in os.listdir(gdrive_path) if f.lower().endswith('.zip')]\n",
        "\n",
        "            if not zip_files_found:\n",
        "                print(f\"‚ùå No .zip files found in '{gdrive_path}'. Please add your file and try again.\")\n",
        "                return None\n",
        "            elif len(zip_files_found) == 1:\n",
        "                chosen_filepath = zip_files_found[0]\n",
        "                print(f\"‚úîÔ∏è Found one ZIP file: '{os.path.basename(chosen_filepath)}'\")\n",
        "                return chosen_filepath\n",
        "            else:\n",
        "                print(f\"\\nMultiple .zip files found. Please choose one:\")\n",
        "                while True:\n",
        "                    for i, filepath in enumerate(zip_files_found):\n",
        "                        print(f\"  {i+1}. {os.path.basename(filepath)}\")\n",
        "                    try:\n",
        "                        choice_str = input(f\"Enter the number of the file you want to use (1-{len(zip_files_found)}): \")\n",
        "                        choice_int = int(choice_str)\n",
        "                        if 1 <= choice_int <= len(zip_files_found):\n",
        "                            chosen_filepath = zip_files_found[choice_int - 1]\n",
        "                            print(f\"‚úîÔ∏è You selected: '{os.path.basename(chosen_filepath)}'\")\n",
        "                            return chosen_filepath\n",
        "                        else:\n",
        "                            print(f\"‚ùå Invalid number. Please enter a number between 1 and {len(zip_files_found)}.\")\n",
        "                    except ValueError:\n",
        "                        print(\"‚ùå Invalid input. Please enter a number.\")\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Please enter 1 or 2.\")\n",
        "\n",
        "# Run the interactive function to get the file path\n",
        "zip_filepath = setup_zip_file_interactive()\n",
        "\n",
        "if zip_filepath:\n",
        "    print(f\"\\n‚úÖ File ready at: {zip_filepath}\")\n",
        "\n",
        "# --- Global Constants needed for this step ---\n",
        "BASE_DIR = \"/content\"\n",
        "EXTRACTED_DATA_DIR = os.path.join(BASE_DIR, \"extracted_data\")\n",
        "instagram_handle = \"unknown_user\" # Default value\n",
        "\n",
        "# --- Helper function updated with your robust JSON parsing logic ---\n",
        "def extract_instagram_handle(personal_info_path):\n",
        "    \"\"\"\n",
        "    Reads the 'personal_information.json' file to find the user's\n",
        "    Instagram username using a safe, step-by-step parsing method.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(personal_info_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- This is your improved, safer navigation logic ---\n",
        "        username_value = None\n",
        "        if isinstance(data, dict) and \"profile_user\" in data and isinstance(data[\"profile_user\"], list) and len(data[\"profile_user\"]) > 0:\n",
        "            # Assuming the relevant data is in the first element of the profile_user list\n",
        "            profile_info = data[\"profile_user\"][0]\n",
        "            if isinstance(profile_info, dict) and \"string_map_data\" in profile_info and isinstance(profile_info[\"string_map_data\"], dict):\n",
        "                string_data = profile_info[\"string_map_data\"]\n",
        "                if \"Username\" in string_data and isinstance(string_data[\"Username\"], dict) and \"value\" in string_data[\"Username\"]:\n",
        "                    username_value = string_data[\"Username\"][\"value\"]\n",
        "        # --- End of your logic ---\n",
        "\n",
        "        if username_value:\n",
        "            return username_value\n",
        "        else:\n",
        "            print(\"  -> Username key not found in the expected structure within the file.\")\n",
        "            return \"unknown_user\"\n",
        "\n",
        "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "        print(f\"  -> Could not read or parse the JSON file. Reason: {e}\")\n",
        "        return \"unknown_user\"\n",
        "\n",
        "# --- Main logic for this cell ---\n",
        "print(\"‚öôÔ∏è Processing your file to find your username...\")\n",
        "\n",
        "if 'zip_filepath' not in globals() or not zip_filepath or not os.path.exists(zip_filepath):\n",
        "    print(\"\\n‚ùå Process stopped. A valid ZIP file was not provided in the previous step.\")\n",
        "    print(\"Please go back to Step 1 and provide a file.\")\n",
        "else:\n",
        "    # Extract the archive\n",
        "    print(f\"--- üìÇ Extracting {os.path.basename(zip_filepath)} ---\")\n",
        "    if os.path.exists(EXTRACTED_DATA_DIR):\n",
        "        shutil.rmtree(EXTRACTED_DATA_DIR)\n",
        "    os.makedirs(EXTRACTED_DATA_DIR, exist_ok=True)\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(EXTRACTED_DATA_DIR)\n",
        "        print(\"‚úîÔ∏è Successfully extracted archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during extraction: {e}\")\n",
        "\n",
        "    # Find the correct path to the JSON file by checking common locations\n",
        "    path_to_check = None\n",
        "    path1 = os.path.join(EXTRACTED_DATA_DIR, 'your_instagram_activity', 'account_information', 'personal_information.json')\n",
        "    path2 = os.path.join(EXTRACTED_DATA_DIR, 'personal_information', 'personal_information.json')\n",
        "    # Add your specific path as a fallback, just in case\n",
        "    path3 = \"/content/extracted_data/personal_information/personal_information/personal_information.json\"\n",
        "\n",
        "\n",
        "    if os.path.exists(path1):\n",
        "        path_to_check = path1\n",
        "    elif os.path.exists(path2):\n",
        "        path_to_check = path2\n",
        "    elif os.path.exists(path3):\n",
        "        path_to_check = path3\n",
        "\n",
        "    if path_to_check:\n",
        "        print(f\"‚úîÔ∏è Found metadata file at: {path_to_check}\")\n",
        "        instagram_handle = extract_instagram_handle(path_to_check)\n",
        "    else:\n",
        "        print(\"‚ùå Could not find 'personal_information.json' in any standard location within the ZIP.\")\n",
        "\n",
        "\n",
        "    if instagram_handle != \"unknown_user\":\n",
        "        print(f\"\\n‚úÖ Username Found: @{instagram_handle}\")\n",
        "        print(\"\\n‚û°Ô∏è You can now run the configuration cell below.\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Could not automatically find your username. Will use a placeholder.\")\n",
        "        print(\"\\n‚û°Ô∏è You can now run the configuration cell below.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilAcRgRKOXO9",
        "outputId": "3daebb49-4083-492a-a480-954266ebe65d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üìÇ How would you like to provide the Instagram ZIP file? ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure Your Instagram to Digital Commons Export\n",
        "#@markdown This form lets you customize the output. Your actual username has been found and is used in the examples below.\n",
        "\n",
        "# --- Display dynamic examples first ---\n",
        "if 'instagram_handle' not in globals() or instagram_handle == 'unknown_user':\n",
        "    ig_handle_for_examples = \"your_username\"\n",
        "    print(\"‚ö†Ô∏è Username not found in the previous step. Using generic examples.\")\n",
        "else:\n",
        "    ig_handle_for_examples = instagram_handle\n",
        "\n",
        "print(\"--- Title Format Examples (using your username) ---\")\n",
        "print(f\"1. Default: Instagram Post by {ig_handle_for_examples} on 2024-08-26\")\n",
        "print(f\"2. Simple:  {ig_handle_for_examples} | Post | 2024-08-26\")\n",
        "print(f\"3. Alt:     Post by {ig_handle_for_examples} (2024-08-26)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "#@markdown ### **1. Content to Process**\n",
        "#@markdown Select which types of Instagram content you want to include in the export.\n",
        "process_posts = True #@param {type:\"boolean\"}\n",
        "process_reels = True #@param {type:\"boolean\"}\n",
        "process_stories = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **2. Title Formatting**\n",
        "#@markdown Choose a title format from the examples printed above.\n",
        "title_format_choice = \"User | Type | Date\" #@param [\"Default (Type by User on Date)\", \"User | Type | Date\", \"Type by User (Date)\", \"Date - User - Type\", \"Custom Format...\"]\n",
        "#@markdown If you chose \"Custom Format...\", define it below using `{username}`, `{doc_type}`, `{doc_type_short}`, and `{date}`.\n",
        "custom_title_template = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **3. Metadata Columns**\n",
        "#@markdown Enter the metadata columns for the final Excel file, separated by commas.\n",
        "#@markdown If you want them, add a comma and any of these: like_count, comments_disabled, latitude, longitude, original_filename, source_file_path\n",
        "metadata_columns_str = \"title, publication_date, abstract, keywords, document_type, fulltext_url, additional_files, instagram_username\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"\\n‚úÖ Configuration loaded.\")\n",
        "print(\"\\n‚û°Ô∏è All settings are ready. Run the final cell below to process your data!\")"
      ],
      "metadata": {
        "id": "KlhJXW62ObKz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get your zips\n",
        "#@markdown Run this and get your downloads.\n",
        "\n",
        "# --- Initial Setup ---\n",
        "!pip install ftfy -q\n",
        "print(\"‚úîÔ∏è 'ftfy' library is installed and ready to help clean up text.\")\n",
        "\n",
        "# --- Core Library Imports ---\n",
        "import os, shutil, zipfile, json, re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "    import ftfy\n",
        "    FTFY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FTFY_AVAILABLE = False\n",
        "\n",
        "# --- Global Constants ---\n",
        "BASE_DIR = \"/content\"\n",
        "EXTRACTED_DATA_DIR = os.path.join(BASE_DIR, \"extracted_data\")\n",
        "BATCHUP_DIR = os.path.join(BASE_DIR, \"batchup\")\n",
        "INSTAGRAM_ACTIVITY_FOLDER_NAME = \"your_instagram_activity\"\n",
        "\n",
        "# ==============================================================================\n",
        "# HELPER FUNCTIONS (Most are already defined, but included for completeness)\n",
        "# ==============================================================================\n",
        "def cleanup_content_directory(filename_to_keep=None):\n",
        "    print(\"üßπ Cleaning up the workspace for a fresh start...\")\n",
        "    items_to_preserve = [\"drive\", \"sample_data\"]\n",
        "    if filename_to_keep:\n",
        "        items_to_preserve.append(os.path.basename(filename_to_keep))\n",
        "    for item in os.listdir(BASE_DIR):\n",
        "        if item in items_to_preserve: continue\n",
        "        item_path = os.path.join(BASE_DIR, item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path): os.unlink(item_path)\n",
        "            elif os.path.isdir(item_path): shutil.rmtree(item_path)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not delete {item_path}. Reason: {e}\")\n",
        "    # Don't print \"Workspace is clean.\" here as it's not the goal of the main script\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return \", \".join(hashtags)\n",
        "\n",
        "def fix_json_encoding(media_json_dir):\n",
        "    if not FTFY_AVAILABLE:\n",
        "        print(\"\\n‚ÑπÔ∏è `ftfy` library not available. Skipping automatic text & emoji fixing.\")\n",
        "        return\n",
        "    print(\"\\n--- üîé Scanning and fixing text encoding in JSON files ---\")\n",
        "    if not os.path.isdir(media_json_dir):\n",
        "        print(f\"‚ùå ERROR: Media JSON directory not found at: {media_json_dir}\")\n",
        "        return\n",
        "    total_files_changed = 0\n",
        "    total_fields_fixed = 0\n",
        "    def _fix_text_in_obj(obj, key):\n",
        "        nonlocal total_fields_fixed\n",
        "        original_text = obj.get(key)\n",
        "        if isinstance(original_text, str) and original_text:\n",
        "            fixed_text = ftfy.fix_text(original_text)\n",
        "            if fixed_text != original_text:\n",
        "                obj[key] = fixed_text; total_fields_fixed += 1; return True\n",
        "        return False\n",
        "    for filename in os.listdir(media_json_dir):\n",
        "        if not filename.lower().endswith('.json'): continue\n",
        "        json_path = os.path.join(media_json_dir, filename)\n",
        "        try:\n",
        "            with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
        "            file_was_changed = False\n",
        "            if 'reels.json' in filename or 'stories.json' in filename:\n",
        "                key = 'ig_reels_media' if 'reels' in filename else 'ig_stories'\n",
        "                if key in data and isinstance(data.get(key), list):\n",
        "                    for item in data[key]:\n",
        "                        for media_item in item.get('media', [item]):\n",
        "                            if isinstance(media_item, dict) and _fix_text_in_obj(media_item, 'title'): file_was_changed = True\n",
        "            elif 'posts_1.json' in filename:\n",
        "                if isinstance(data, list):\n",
        "                    for post in data:\n",
        "                        if isinstance(post, dict):\n",
        "                            if _fix_text_in_obj(post, 'title'): file_was_changed = True\n",
        "                            for media_item in post.get('media', []):\n",
        "                                 if isinstance(media_item, dict) and _fix_text_in_obj(media_item, 'title'): file_was_changed = True\n",
        "            if file_was_changed:\n",
        "                total_files_changed += 1\n",
        "                with open(json_path, 'w', encoding='utf-8') as f: json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  üîß Fixed text/emojis in: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not process {filename}. Reason: {e}\")\n",
        "    print(f\"‚úîÔ∏è Text fixing complete. Total fields fixed: {total_fields_fixed} in {total_files_changed} files.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE PROCESSING FUNCTION\n",
        "# ==============================================================================\n",
        "def process_media_type(media_type, json_filename, username, selected_columns, title_format_choice_tuple):\n",
        "    print(f\"\\n{'='*20} PROCESSING: {media_type.upper()} {'='*20}\")\n",
        "    output_dir = os.path.join(EXTRACTED_DATA_DIR, f'{media_type}_export_dc_format')\n",
        "    if os.path.exists(output_dir): shutil.rmtree(output_dir) # Clean previous runs for this type\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"‚úîÔ∏è Created output directory: {output_dir}\")\n",
        "    json_path = os.path.join(EXTRACTED_DATA_DIR, INSTAGRAM_ACTIVITY_FOLDER_NAME, 'media', json_filename)\n",
        "    excel_path = os.path.join(output_dir, f'{media_type}_metadata.xlsx')\n",
        "    readme_path = os.path.join(output_dir, 'README.txt')\n",
        "    final_zip_path_in_batchup = os.path.join(BATCHUP_DIR, f\"{media_type}.zip\")\n",
        "    package_zip_path_in_output = os.path.join(output_dir, f'{media_type}_package.zip')\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        print(f\"‚ÑπÔ∏è Could not find or read {json_filename}. Skipping this media type.\")\n",
        "        return\n",
        "    items_to_process = []\n",
        "    if media_type == 'posts': items_to_process = data\n",
        "    elif media_type == 'reels': items_to_process = data.get('ig_reels_media', [])\n",
        "    elif media_type == 'stories': items_to_process = data.get('ig_stories', [])\n",
        "    if not items_to_process:\n",
        "        print(f\"‚ÑπÔ∏è No items found for '{media_type}'. Nothing to process.\")\n",
        "        return\n",
        "    items_to_process.sort(key=lambda x: x.get('creation_timestamp', float('inf')))\n",
        "    print(f\"‚úîÔ∏è Found {len(items_to_process)} total {media_type} items to process.\")\n",
        "    excel_data_rows = []\n",
        "    copied_files_for_zip = []\n",
        "    skipped_counts = {'no_uri': 0, 'web_link': 0, 'file_missing': 0, 'copy_error': 0}\n",
        "    for item_index, item in enumerate(items_to_process):\n",
        "        media_list = item.get('media', [item])\n",
        "        post_level_timestamp = item.get('creation_timestamp', media_list[0].get('creation_timestamp'))\n",
        "        if not post_level_timestamp: continue\n",
        "        post_level_caption = item.get('title', media_list[0].get('title', ''))\n",
        "        for media_index, media_item in enumerate(media_list):\n",
        "            if not isinstance(media_item, dict): continue\n",
        "            original_uri = media_item.get('uri')\n",
        "            if not original_uri or original_uri.startswith(('http://', 'https://')):\n",
        "                skipped_counts['no_uri'] += 1; continue\n",
        "            source_media_path = os.path.join(EXTRACTED_DATA_DIR, original_uri)\n",
        "            if not os.path.exists(source_media_path):\n",
        "                skipped_counts['file_missing'] += 1; continue\n",
        "            date_obj = datetime.fromtimestamp(post_level_timestamp)\n",
        "            ext = os.path.splitext(original_uri)[-1]\n",
        "            original_filename_base = os.path.basename(original_uri)\n",
        "            sanitized_basename = re.sub(r'[^a-zA-Z0-9_-]', '_', os.path.splitext(original_filename_base)[0])\n",
        "            new_media_filename = f\"instagram_{username}_{media_type}_{date_obj.strftime('%Y-%m-%d')}_post{item_index+1}_media{media_index+1}_{sanitized_basename}{ext}\"\n",
        "            dest_media_path = os.path.join(output_dir, new_media_filename)\n",
        "            try:\n",
        "                shutil.copy2(source_media_path, dest_media_path)\n",
        "                copied_files_for_zip.append(dest_media_path)\n",
        "            except Exception as e:\n",
        "                skipped_counts['copy_error'] += 1; continue\n",
        "            additional_files_str = ''\n",
        "            if media_type == 'reels':\n",
        "                subtitles_uri = media_item.get('media_metadata', {}).get('video_metadata', {}).get('subtitles', {}).get('uri')\n",
        "                if subtitles_uri and os.path.exists(os.path.join(EXTRACTED_DATA_DIR, subtitles_uri)):\n",
        "                    source_srt_path = os.path.join(EXTRACTED_DATA_DIR, subtitles_uri)\n",
        "                    new_srt_filename = os.path.splitext(new_media_filename)[0] + '.srt'\n",
        "                    dest_srt_path = os.path.join(output_dir, new_srt_filename)\n",
        "                    shutil.copy2(source_srt_path, dest_srt_path)\n",
        "                    copied_files_for_zip.append(dest_srt_path)\n",
        "                    additional_files_str = new_srt_filename\n",
        "            location_info = media_item.get('location', {})\n",
        "            row_data = {\n",
        "                'title': \"Placeholder Title\", 'publication_date': date_obj.strftime('%Y-%m-%d'),\n",
        "                'abstract': post_level_caption, 'keywords': extract_hashtags(post_level_caption),\n",
        "                'document_type': f\"Instagram {media_type.capitalize()}\", 'fulltext_url': new_media_filename,\n",
        "                'additional_files': additional_files_str, 'instagram_username': username,\n",
        "                'like_count': media_item.get('like_count', ''), 'comments_disabled': item.get('comments_disabled', False),\n",
        "                'latitude': location_info.get('lat', ''), 'longitude': location_info.get('lng', ''),\n",
        "                'original_filename': original_filename_base, 'source_file_path': original_uri,\n",
        "                'creation_timestamp': post_level_timestamp,\n",
        "            }\n",
        "            excel_data_rows.append(row_data)\n",
        "    if not excel_data_rows: print(f\"‚ÑπÔ∏è No processable media items found for {media_type}.\"); return\n",
        "    df = pd.DataFrame(excel_data_rows)\n",
        "    choice, custom_template = title_format_choice_tuple\n",
        "    new_titles = []\n",
        "    for _, row in df.iterrows():\n",
        "        template_map = {'Default (Type by User on Date)': \"{doc_type} by {username} on {date}\", 'User | Type | Date': \"{username} | {doc_type_short} | {date}\", 'Type by User (Date)': \"{doc_type_short} by {username} ({date})\", 'Date - User - Type': \"{date} - {username} - {doc_type_short}\", 'Custom Format...': custom_template or \"{doc_type} by {username} on {date}\"}\n",
        "        template_str = template_map.get(choice)\n",
        "        date_str = datetime.fromtimestamp(row['creation_timestamp']).strftime('%Y-%m-%d')\n",
        "        doc_type_short = row['document_type'].replace(\"Instagram \", \"\")\n",
        "        new_titles.append(template_str.format(username=row['instagram_username'], doc_type=row['document_type'], doc_type_short=doc_type_short, date=date_str))\n",
        "    df['title'] = new_titles\n",
        "    df['_date_str'] = df['creation_timestamp'].apply(lambda ts: datetime.fromtimestamp(ts).strftime('%Y-%m-%d'))\n",
        "    df['_date_rank'] = df.groupby('_date_str').cumcount() + 1\n",
        "    date_counts = df['_date_str'].value_counts()\n",
        "    multi_post_dates = date_counts[date_counts > 1].index\n",
        "    if not multi_post_dates.empty: df['title'] = df.apply(lambda row: f\"{row['title']} - {row['_date_rank']} of {date_counts[row['_date_str']]}\" if row['_date_str'] in multi_post_dates else row['title'], axis=1)\n",
        "    df.drop(columns=['_date_str', '_date_rank', 'creation_timestamp'], inplace=True, errors='ignore')\n",
        "    final_df = pd.DataFrame(df, columns=[col for col in selected_columns if col in df.columns])\n",
        "    final_df.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "    print(f\"‚úîÔ∏è Metadata for {len(final_df)} items written to {os.path.basename(excel_path)}\")\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f: f.write(f\"Instagram {media_type.capitalize()} Export Package\\n{'='*40}\\n\\nHandle: @{username}\\nExported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\nThis package contains {len(final_df)} exported media items and their metadata.\\n\\n--- SKIPPED ITEMS SUMMARY ---\\n  - No media file path in JSON: {skipped_counts['no_uri']}\\n  - Item was a web link (not local): {skipped_counts['web_link']}\\n  - Local media file was missing: {skipped_counts['file_missing']}\\n  - Error copying file: {skipped_counts['copy_error']}\\n\\nGenerated by instagram2digitalcommons script.\\n\")\n",
        "    with zipfile.ZipFile(package_zip_path_in_output, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(excel_path, arcname=os.path.basename(excel_path))\n",
        "        zipf.write(readme_path, arcname='README.txt')\n",
        "        for file_path in copied_files_for_zip: zipf.write(file_path, arcname=os.path.basename(file_path))\n",
        "    if not os.path.exists(BATCHUP_DIR): os.makedirs(BATCHUP_DIR)\n",
        "    shutil.copy2(package_zip_path_in_output, final_zip_path_in_batchup)\n",
        "    print(f\"‚úîÔ∏è Final package '{os.path.basename(final_zip_path_in_batchup)}' is ready.\")\n",
        "    files.download(final_zip_path_in_batchup)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION SCRIPT\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ Starting the Instagram to Digital Commons Exporter! üöÄ\")\n",
        "    print(\"=\" * 60)\n",
        "    if 'zip_filepath' not in globals() or not zip_filepath or not os.path.exists(zip_filepath):\n",
        "        print(\"\\n‚ùå Process stopped. A valid ZIP file was not provided in Step 1.\")\n",
        "        return\n",
        "    if 'instagram_handle' not in globals():\n",
        "        print(\"\\n‚ùå Process stopped. Username was not identified in Step 2.\")\n",
        "        return\n",
        "    selected_cols = [col.strip() for col in metadata_columns_str.split(',') if col.strip()]\n",
        "    title_choice_tuple = (title_format_choice, custom_title_template)\n",
        "    print(\"--- Settings Confirmed ---\")\n",
        "    print(f\"Username: @{instagram_handle}\")\n",
        "    print(f\"Title Format: {title_format_choice}\")\n",
        "    print(f\"Processing Posts: {process_posts}, Reels: {process_reels}, Stories: {process_stories}\")\n",
        "    print(\"-\" * 26)\n",
        "    media_json_dir = os.path.join(EXTRACTED_DATA_DIR, INSTAGRAM_ACTIVITY_FOLDER_NAME, 'media')\n",
        "    fix_json_encoding(media_json_dir)\n",
        "    if process_posts: process_media_type('posts', 'posts_1.json', instagram_handle, selected_cols, title_choice_tuple)\n",
        "    if process_reels: process_media_type('reels', 'reels.json', instagram_handle, selected_cols, title_choice_tuple)\n",
        "    if process_stories: process_media_type('stories', 'stories.json', instagram_handle, selected_cols, title_choice_tuple)\n",
        "    print(\"\\nüéâ All processing complete! üéâ\")\n",
        "    print(f\"Your final ZIP packages can be found in the '{os.path.basename(BATCHUP_DIR)}' folder in the Files panel on the left.\")\n",
        "\n",
        "# --- Run the main function ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "2BPs97MkQoSV",
        "outputId": "fe038f83-239c-4ae5-9eae-d30966bc450a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è 'ftfy' library is installed and ready to help clean up text.\n",
            "============================================================\n",
            "üöÄ Starting the Instagram to Digital Commons Exporter! üöÄ\n",
            "============================================================\n",
            "--- Settings Confirmed ---\n",
            "Username: @umsllibraries\n",
            "Title Format: User | Type | Date\n",
            "Processing Posts: True, Reels: True, Stories: True\n",
            "--------------------------\n",
            "\n",
            "--- üîé Scanning and fixing text encoding in JSON files ---\n",
            "  üîß Fixed text/emojis in: stories.json\n",
            "  üîß Fixed text/emojis in: reels.json\n",
            "  üîß Fixed text/emojis in: posts_1.json\n",
            "‚úîÔ∏è Text fixing complete. Total fields fixed: 262 in 3 files.\n",
            "--------------------------------------------------\n",
            "\n",
            "==================== PROCESSING: POSTS ====================\n",
            "‚úîÔ∏è Created output directory: /content/extracted_data/posts_export_dc_format\n",
            "‚úîÔ∏è Found 646 total posts items to process.\n",
            "‚úîÔ∏è Metadata for 860 items written to posts_metadata.xlsx\n",
            "‚úîÔ∏è Final package 'posts.zip' is ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0618d34d-bd8b-4865-a2d2-4b35754a9e5f\", \"posts.zip\", 150032118)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "==================== PROCESSING: REELS ====================\n",
            "‚úîÔ∏è Created output directory: /content/extracted_data/reels_export_dc_format\n",
            "‚úîÔ∏è Found 25 total reels items to process.\n",
            "‚úîÔ∏è Metadata for 25 items written to reels_metadata.xlsx\n",
            "‚úîÔ∏è Final package 'reels.zip' is ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4609744b-83fa-40b3-a43d-c6ec6f4cfceb\", \"reels.zip\", 154618940)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "==================== PROCESSING: STORIES ====================\n",
            "‚úîÔ∏è Created output directory: /content/extracted_data/stories_export_dc_format\n",
            "‚úîÔ∏è Found 485 total stories items to process.\n",
            "‚úîÔ∏è Metadata for 450 items written to stories_metadata.xlsx\n",
            "‚úîÔ∏è Final package 'stories.zip' is ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0dab4c56-bf59-4bda-a3a2-4fc48966b505\", \"stories.zip\", 60390438)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "üéâ All processing complete! üéâ\n",
            "Your final ZIP packages can be found in the 'batchup' folder in the Files panel on the left.\n"
          ]
        }
      ]
    }
  ]
}