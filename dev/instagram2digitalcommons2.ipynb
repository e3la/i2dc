{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWaqck4j3jp9DR2UqXmcdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e3la/i2dc/blob/main/instagram2digitalcommons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to instagram2digitalcommons!\n",
        "\n",
        "This Colab notebook is designed to help you transform an Instagram archive (the `.zip` file you download from Instagram) into structured packages that are easier to ingest into a Digital Commons (or similar) institutional repository.\n",
        "\n",
        "This tool was created with vibecoding. The workflow was varied but the most successful version was downloading the ipynb file from colab uploading it to aistudio gemini and asking to add the next cell, and playing with it until it worked, and repeating it for each piece of the code.\n",
        "\n",
        "**What you need to do:**\n",
        "\n",
        "1.  **Have a google account:**\n",
        "    *   To run this you'll need to log into a google account, or know enough magic to run this pile of python somewhere else.\n",
        "2.  **Provide your Instagram Archive:**\n",
        "    *   The first code cell will ask you how you want to provide your Instagram `.zip` file. You can either:\n",
        "        *   Upload it directly to this Colab session (suitable for smaller archives).\n",
        "        *   Place it in a specific folder (`MyDrive/i2dc/`) on your Google Drive and let the script find it (recommended for larger archives or repeated use).\n",
        "    *   Make sure you have your Instagram archive `.zip` file ready. You can request it from Instagram by going to your Profile -> Your Activity -> Download Your Information. **Crucially, request the data in JSON format and select High media quality.**\n",
        "\n",
        "3.  **Run the Cells Sequentially:**\n",
        "    *   Execute each code cell in this notebook from top to bottom by clicking the \"play\" button next to each cell or by using \"Runtime\" > \"Run all\".\n",
        "    *   The notebook will:\n",
        "        *   Extract the contents of your `.zip` file.\n",
        "        *   Scan the archive and provide a summary of its contents.\n",
        "        *   Attempt to fix common text encoding issues (mojibake) in captions and titles using `ftfy`.\n",
        "        *   Process **Reels**, **Stories**, and **Posts** separately.\n",
        "\n",
        "**What the notebook will produce:**\n",
        "\n",
        "For each content type (Reels, Stories, Posts), the notebook will:\n",
        "\n",
        "1.  **Create a dedicated export folder** within the Colab environment (e.g., `/content/extracted_data/reels_export_dc_format/`).\n",
        "2.  **Copy and rename relevant media files** (videos, images) into this folder. For Reels, it will also include `.srt` subtitle files if they exist in your archive. Filenames are structured for clarity (e.g., `instagram_yourhandle_reel_YYYY-MM-DD_item-number_original-name.mp4`).\n",
        "3.  **Generate an Excel metadata file** (e.g., `reels_metadata_dc_format.xlsx`). This file is formatted with columns typically used for Digital Commons batch uploads (`title`, `fulltext_url`, `keywords`, `abstract`, etc.).\n",
        "    *   For **Posts** containing multiple media items (carousels), each media item will get its own row in the Excel sheet, sharing common post metadata but having a unique `fulltext_url`.\n",
        "    *   For **Stories** posted at the same time (grouped by timestamp), they will share common metadata like `title` and `abstract`, but each media item will have its own row and unique `fulltext_url`.\n",
        "4.  **Create a `README.txt` file** in the export folder, explaining the contents of that specific package, how metadata was generated, and a summary of processed/skipped items.\n",
        "5.  **Package these items into a single `.zip` file** (e.g., `reels_package_dc_format.zip`) within its export folder.\n",
        "6.  **Copy this package to `/content/batchup/`** and rename it (e.g., `reels.zip`, `stories.zip`, `posts.zip`).\n",
        "7.  **Offer this final `.zip` file for download** directly to your computer after each respective section (Reels, Stories, Posts) is processed.\n",
        "\n",
        "You will end up with three main downloadable `.zip` files: `reels.zip`, `stories.zip`, and `posts.zip`, each ready for further review and potential batch upload to Digital Commons.\n",
        "\n",
        "**Important Notes:**\n",
        "*   This notebook processes media files that are *locally available* within your downloaded Instagram archive. It does **not** download content from web links (e.g., some older story links might point to Instagram's servers). The script attempts to skip these web links.\n",
        "*   The quality of the output depends on the completeness and structure of your Instagram archive.\n",
        "*   Always review the generated metadata and files before uploading to your repository.\n",
        "\n",
        "Let's get started! Run the first code cell below (the one that asks how you want to provide the ZIP file)."
      ],
      "metadata": {
        "id": "lJz0rlIMnftE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxMQIIdgkj6x",
        "outputId": "5700dd64-452b-4d35-e490-10815f33ad25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "How do you want to provide the ZIP file?\n",
            "1. Upload directly to Colab (for smaller files, keeps original name).\n",
            "2. Use the ZIP file from Google Drive (searches 'MyDrive/i2dc/' for a unique .zip file).\n",
            "Enter choice (1 or 2): 2\n",
            "--------------------------------------------------\n",
            "Selected: Use ZIP file from Google Drive.\n",
            "Attempting to mount Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully at /content/drive\n",
            "Searching for a unique .zip file in: '/content/drive/MyDrive/i2dc'\n",
            "Found ZIP file: 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'\n",
            "File path in Colab (via Drive): /content/drive/MyDrive/i2dc/instagram-umsllibraries-2025-03-07-Ardjbhx1.zip\n",
            "--------------------------------------------------\n",
            "\n",
            "Proceeding with ZIP file: 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'\n",
            "Located at: /content/drive/MyDrive/i2dc/instagram-umsllibraries-2025-03-07-Ardjbhx1.zip\n",
            "Cleaning up previous extraction at '/content/extracted_data'...\n",
            "Extraction target directory: '/content/extracted_data'\n",
            "Attempting to extract 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip'...\n",
            "Successfully extracted 'instagram-umsllibraries-2025-03-07-Ardjbhx1.zip' to '/content/extracted_data'\n",
            "\n",
            "Contents of the extracted folder:\n",
            "- logged_information\n",
            "- security_and_login_information\n",
            "- media\n",
            "- apps_and_websites_off_of_instagram\n",
            "- preferences\n",
            "- personal_information\n",
            "- connections\n",
            "- ads_information\n",
            "- your_instagram_activity\n",
            "--------------------------------------------------\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil # For file operations like deleting directories\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "\n",
        "zip_filepath = None # This will store the path to the zip file\n",
        "uploaded_filename_original = None # To store the original name from upload\n",
        "\n",
        "# --- Helper function to clean up previous uploads from /content/ if method 1 is chosen ---\n",
        "def cleanup_content_directory(filename_to_keep=None):\n",
        "    \"\"\"Removes all files and folders from /content/ except specified ones.\"\"\"\n",
        "    print(\"Cleaning up /content/ directory...\")\n",
        "    items_to_preserve = [\"drive\", \"sample_data\"] # Default Colab folders\n",
        "    if filename_to_keep:\n",
        "        items_to_preserve.append(os.path.basename(filename_to_keep))\n",
        "\n",
        "    for item in os.listdir(\"/content/\"):\n",
        "        if item in items_to_preserve:\n",
        "            continue\n",
        "        item_path = os.path.join(\"/content/\", item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)\n",
        "                # print(f\"Removed file: {item_path}\")\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "                # print(f\"Removed directory: {item_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {item_path}. Reason: {e}\")\n",
        "    print(\"Cleanup of /content/ complete.\")\n",
        "\n",
        "# --- Ask the user how they want to provide the file ---\n",
        "while True:\n",
        "    print(\"-\" * 50)\n",
        "    method = input(\n",
        "        \"How do you want to provide the ZIP file?\\n\"\n",
        "        \"1. Upload directly to Colab (for smaller files, keeps original name).\\n\"\n",
        "        \"2. Use the ZIP file from Google Drive (searches 'MyDrive/i2dc/' for a unique .zip file).\\n\"\n",
        "        \"Enter choice (1 or 2): \"\n",
        "    ).strip()\n",
        "    if method in ['1', '2']:\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Option 1: Direct Upload ---\n",
        "if method == '1':\n",
        "    print(\"Selected: Upload directly to Colab.\")\n",
        "    print(\"Please wait for the upload dialog and select your ZIP file...\")\n",
        "\n",
        "    # Clean up /content/ before new upload\n",
        "    cleanup_content_directory()\n",
        "\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"No file was uploaded. Exiting.\")\n",
        "        else:\n",
        "            # Get the uploaded file name (key in the 'uploaded' dict)\n",
        "            uploaded_filename_original = list(uploaded.keys())[0]\n",
        "\n",
        "            if not uploaded_filename_original.lower().endswith('.zip'):\n",
        "                print(f\"Error: The uploaded file '{uploaded_filename_original}' is not a ZIP file.\")\n",
        "                # Clean up the wrongly uploaded file\n",
        "                wrong_file_path = os.path.join(\"/content/\", uploaded_filename_original)\n",
        "                if os.path.exists(wrong_file_path):\n",
        "                    os.remove(wrong_file_path)\n",
        "                uploaded_filename_original = None # Reset as it's not a valid zip\n",
        "            else:\n",
        "                # The file is uploaded directly to /content/\n",
        "                zip_filepath = os.path.join(\"/content/\", uploaded_filename_original)\n",
        "                print(f\"Successfully uploaded: '{uploaded_filename_original}'\")\n",
        "                print(f\"File path in Colab: {zip_filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during upload: {e}\")\n",
        "        print(\"If the file is too large, please try the Google Drive option next time.\")\n",
        "\n",
        "# --- Option 2: Google Drive ---\n",
        "elif method == '2':\n",
        "    print(\"Selected: Use ZIP file from Google Drive.\")\n",
        "    print(\"Attempting to mount Google Drive...\")\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully at /content/drive\")\n",
        "\n",
        "        gdrive_my_drive_path = \"/content/drive/MyDrive/\"\n",
        "        target_folder_name = \"i2dc\"\n",
        "        target_folder_path_in_drive = os.path.join(gdrive_my_drive_path, target_folder_name)\n",
        "\n",
        "        print(f\"Searching for a unique .zip file in: '{target_folder_path_in_drive}'\")\n",
        "\n",
        "        if not os.path.isdir(target_folder_path_in_drive):\n",
        "            print(f\"Error: The folder '{target_folder_path_in_drive}' ('{target_folder_name}' in your MyDrive) does not exist.\")\n",
        "            print(f\"Please ensure you have a folder named '{target_folder_name}' directly under 'My Drive' containing your .zip file.\")\n",
        "        else:\n",
        "            zip_files_found = []\n",
        "            for item_name in os.listdir(target_folder_path_in_drive):\n",
        "                item_full_path = os.path.join(target_folder_path_in_drive, item_name)\n",
        "                if os.path.isfile(item_full_path) and item_name.lower().endswith('.zip'):\n",
        "                    zip_files_found.append(item_full_path)\n",
        "\n",
        "            if len(zip_files_found) == 0:\n",
        "                print(f\"No .zip files found in '{target_folder_path_in_drive}'.\")\n",
        "            elif len(zip_files_found) == 1:\n",
        "                zip_filepath = zip_files_found[0]\n",
        "                uploaded_filename_original = os.path.basename(zip_filepath) # Get original name from path\n",
        "                print(f\"Found ZIP file: '{uploaded_filename_original}'\")\n",
        "                print(f\"File path in Colab (via Drive): {zip_filepath}\")\n",
        "            else:\n",
        "                print(f\"Error: Multiple .zip files found in '{target_folder_path_in_drive}':\")\n",
        "                for f_path in zip_files_found:\n",
        "                    print(f\" - {os.path.basename(f_path)}\")\n",
        "                print(\"Please ensure there is only one .zip file in that folder for this option to work automatically.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while mounting or accessing Google Drive: {e}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Proceed with the zip file if one was successfully identified ---\n",
        "if zip_filepath and os.path.exists(zip_filepath):\n",
        "    print(f\"\\nProceeding with ZIP file: '{uploaded_filename_original}'\")\n",
        "    print(f\"Located at: {zip_filepath}\")\n",
        "\n",
        "    # --- Your next steps using zip_filepath ---\n",
        "    extract_to_folder = \"/content/extracted_data\" # Define your extraction path\n",
        "\n",
        "    # Clean up previous extraction if it exists\n",
        "    if os.path.exists(extract_to_folder):\n",
        "        print(f\"Cleaning up previous extraction at '{extract_to_folder}'...\")\n",
        "        shutil.rmtree(extract_to_folder)\n",
        "    os.makedirs(extract_to_folder, exist_ok=True)\n",
        "    print(f\"Extraction target directory: '{extract_to_folder}'\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Attempting to extract '{uploaded_filename_original}'...\")\n",
        "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_folder)\n",
        "        print(f\"Successfully extracted '{uploaded_filename_original}' to '{extract_to_folder}'\")\n",
        "\n",
        "        print(\"\\nContents of the extracted folder:\")\n",
        "        extracted_items = os.listdir(extract_to_folder)\n",
        "        if not extracted_items:\n",
        "            print(\"(The extracted folder is empty)\")\n",
        "        else:\n",
        "            for item in extracted_items:\n",
        "                print(f\"- {item}\")\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: The file '{uploaded_filename_original}' is not a valid ZIP file or is corrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "elif method == '1' and not zip_filepath and uploaded_filename_original is None and 'uploaded' in locals() and not uploaded:\n",
        "    # This case is already handled by \"No file was uploaded.\"\n",
        "    pass\n",
        "elif method == '1' and not zip_filepath and uploaded_filename_original is None:\n",
        "    # This case is for \"uploaded file was not a ZIP\"\n",
        "    print(\"\\nCannot proceed as the uploaded file was not a valid ZIP file.\")\n",
        "elif not zip_filepath:\n",
        "    print(\"\\nNo ZIP file was successfully specified or found. Cannot proceed with further operations.\")\n",
        "elif not os.path.exists(zip_filepath): # Should be rare if logic above is correct\n",
        "     print(f\"\\nError: The determined ZIP file path '{zip_filepath}' does not seem to exist. This is unexpected. Cannot proceed.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Script finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Revised: Inspect contents of the Instagram archive using your file counting logic\n",
        "import os\n",
        "\n",
        "MEDIA_DIR = \"/content/extracted_data\"\n",
        "\n",
        "print(f\"\\nüîé Scanning '{MEDIA_DIR}' for Instagram data...\")\n",
        "\n",
        "file_count = 0\n",
        "image_count = 0\n",
        "video_count = 0\n",
        "\n",
        "for root, dirs, files in os.walk(MEDIA_DIR):\n",
        "    for file in files:\n",
        "        file_count += 1\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):\n",
        "            image_count += 1\n",
        "        elif file.lower().endswith('.mp4'):\n",
        "            video_count += 1\n",
        "\n",
        "print(\"\\nüìä Archive Summary\")\n",
        "print(\"-----------------------------------------\")\n",
        "print(f\"Total files: {file_count}\")\n",
        "print(f\"Image files: {image_count}\")\n",
        "print(f\"MP4 video files: {video_count}\")\n",
        "print(\"You can also browse these files using the 'Files' panel on the left sidebar.\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "# üì¶ Count Instagram media types from extracted JSON data\n",
        "import json\n",
        "\n",
        "# Base directory: previously defined as extract_dir\n",
        "base_dir = MEDIA_DIR\n",
        "activity_dir = None\n",
        "\n",
        "# Base directory: previously defined as extract_dir\n",
        "base_dir = MEDIA_DIR\n",
        "\n",
        "# Default activity folder name\n",
        "activity_dir = 'your_instagram_activity'\n",
        "print(f\"üìÅ Using default Instagram activity folder: '{activity_dir}'\")\n",
        "\n",
        "media_json_path = os.path.join(base_dir, activity_dir, 'media')\n",
        "print(f\"\\nüìÇ Searching media JSON files in: {media_json_path}\")\n",
        "# Define target files\n",
        "json_files = {\n",
        "    \"posts\": \"posts_1.json\",\n",
        "    \"reels\": \"reels.json\",\n",
        "    \"stories\": \"stories.json\"\n",
        "}\n",
        "\n",
        "counts = {}\n",
        "\n",
        "for media_type, filename in json_files.items():\n",
        "    file_path = os.path.join(media_json_path, filename)\n",
        "    print(f\"\\nüîç {media_type.capitalize()}: {filename}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(\"   ‚ö†Ô∏è File not found.\")\n",
        "            counts[media_type] = 0\n",
        "            continue\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if isinstance(data, list):  # posts_1.json\n",
        "            counts[media_type] = len(data)\n",
        "            print(f\"   ‚úÖ Found {counts[media_type]} entries (flat list).\")\n",
        "\n",
        "        elif isinstance(data, dict):\n",
        "            top_key = {\n",
        "                \"reels\": \"ig_reels_media\",\n",
        "                \"stories\": \"ig_stories\"\n",
        "            }.get(media_type)\n",
        "\n",
        "            if top_key not in data:\n",
        "                print(f\"   ‚ö†Ô∏è Key '{top_key}' not found.\")\n",
        "                counts[media_type] = 0\n",
        "                continue\n",
        "\n",
        "            flat_list = []\n",
        "            for group in data[top_key]:\n",
        "                if isinstance(group, dict) and \"media\" in group:\n",
        "                    flat_list.extend(group[\"media\"])\n",
        "                else:\n",
        "                    flat_list.append(group)\n",
        "\n",
        "            counts[media_type] = len(flat_list)\n",
        "            print(f\"   ‚úÖ Found {counts[media_type]} entries (flattened).\")\n",
        "\n",
        "        else:\n",
        "            print(\"   ‚ùå Unrecognized JSON structure.\")\n",
        "            counts[media_type] = 0\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"   ‚ùå Error decoding JSON.\")\n",
        "        counts[media_type] = 0\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Unexpected error: {e}\")\n",
        "        counts[media_type] = 0\n",
        "\n",
        "# prompt: this is in the json\n",
        "# {\n",
        "#   \"profile_user\": [\n",
        "#     {\n",
        "...\n",
        "#         \"Username\": {\n",
        "#           \"href\": \"\",\n",
        "#           \"value\": \"umsllibraries\",\n",
        "#           \"timestamp\": 0\n",
        "# in the file /content/extracted_data/personal_information/personal_information/personal_information.json\n",
        "# and I want to pull out just the Username value\n",
        "\n",
        "# Path to the JSON file\n",
        "json_file_path = \"/content/extracted_data/personal_information/personal_information/personal_information.json\"\n",
        "\n",
        "# Load the JSON data\n",
        "try:\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Navigate through the structure to find the Username value\n",
        "    username_value = None\n",
        "    if isinstance(data, dict) and \"profile_user\" in data and isinstance(data[\"profile_user\"], list) and len(data[\"profile_user\"]) > 0:\n",
        "        # Assuming the relevant data is in the first element of the profile_user list\n",
        "        profile_info = data[\"profile_user\"][0]\n",
        "        if isinstance(profile_info, dict) and \"string_map_data\" in profile_info and isinstance(profile_info[\"string_map_data\"], dict):\n",
        "            string_data = profile_info[\"string_map_data\"]\n",
        "            if \"Username\" in string_data and isinstance(string_data[\"Username\"], dict) and \"value\" in string_data[\"Username\"]:\n",
        "                username_value = string_data[\"Username\"][\"value\"]\n",
        "\n",
        "    # Print the extracted username\n",
        "    if username_value is not None:\n",
        "        print(f\"\\nExtracted Username: {username_value}\")\n",
        "    else:\n",
        "        print(\"Username not found in the expected structure.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {json_file_path}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from {json_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjEQbVszpZ6s",
        "outputId": "2c0e3691-adf5-40a6-b878-1bdd1dcdb099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Scanning '/content/extracted_data' for Instagram data...\n",
            "\n",
            "üìä Archive Summary\n",
            "-----------------------------------------\n",
            "Total files: 1445\n",
            "Image files: 1229\n",
            "MP4 video files: 103\n",
            "You can also browse these files using the 'Files' panel on the left sidebar.\n",
            "-----------------------------------------\n",
            "üìÅ Using default Instagram activity folder: 'your_instagram_activity'\n",
            "\n",
            "üìÇ Searching media JSON files in: /content/extracted_data/your_instagram_activity/media\n",
            "\n",
            "üîç Posts: posts_1.json\n",
            "   ‚úÖ Found 646 entries (flat list).\n",
            "\n",
            "üîç Reels: reels.json\n",
            "   ‚úÖ Found 25 entries (flattened).\n",
            "\n",
            "üîç Stories: stories.json\n",
            "   ‚úÖ Found 485 entries (flattened).\n",
            "\n",
            "Extracted Username: umsllibraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO RUN BEFORE CONTENT PROCESSING CELLS\n",
        "# This cell will iterate through all .json files in the media directory,\n",
        "# back them up, and attempt to fix mojibake in known text fields using ftfy.\n",
        "\n",
        "# 1. Install ftfy\n",
        "!pip install ftfy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil # For copying files\n",
        "import ftfy\n",
        "\n",
        "print(\"--- Starting JSON fixing process for all media files ---\")\n",
        "\n",
        "# Define the path to the media directory\n",
        "MEDIA_DIR_BASE = \"/content/extracted_data\" # Make sure this matches your setup\n",
        "TARGET_MEDIA_DIR = os.path.join(MEDIA_DIR_BASE, 'your_instagram_activity', 'media')\n",
        "\n",
        "if not os.path.isdir(TARGET_MEDIA_DIR):\n",
        "    print(f\"‚ùå ERROR: Media directory not found at: {TARGET_MEDIA_DIR}\")\n",
        "    print(\"Please ensure the extraction in the first cell was successful and the path is correct.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found media directory: {TARGET_MEDIA_DIR}\")\n",
        "\n",
        "    total_files_processed = 0\n",
        "    total_files_changed_by_ftfy = 0\n",
        "    total_titles_fixed_overall = 0\n",
        "\n",
        "    # Helper function to fix text fields in common Instagram JSON structures\n",
        "    def fix_text_fields_in_data(loaded_data, filename_basename):\n",
        "        \"\"\"\n",
        "        Modifies loaded_data in-place by applying ftfy.fix_text to known text fields.\n",
        "        Returns: (bool: data_was_changed, int: titles_fixed_count)\n",
        "        \"\"\"\n",
        "        data_was_changed_locally = False\n",
        "        titles_fixed_count_locally = 0\n",
        "\n",
        "        def _apply_fix(obj, key):\n",
        "            nonlocal data_was_changed_locally, titles_fixed_count_locally\n",
        "            original_text = obj.get(key)\n",
        "            if isinstance(original_text, str) and original_text: # Only fix non-empty strings\n",
        "                fixed_text = ftfy.fix_text(original_text)\n",
        "                if fixed_text != original_text:\n",
        "                    obj[key] = fixed_text\n",
        "                    data_was_changed_locally = True\n",
        "                    titles_fixed_count_locally += 1\n",
        "\n",
        "        if filename_basename == \"reels.json\" or filename_basename == \"stories.json\":\n",
        "            top_level_key = \"ig_reels_media\" if filename_basename == \"reels.json\" else \"ig_stories\"\n",
        "            if isinstance(loaded_data, dict) and top_level_key in loaded_data:\n",
        "                item_list = loaded_data.get(top_level_key, [])\n",
        "                for item_dict in item_list: # Can be group or direct media item\n",
        "                    if isinstance(item_dict, dict):\n",
        "                        if \"media\" in item_dict and isinstance(item_dict[\"media\"], list): # It's a group\n",
        "                            for media_sub_item_dict in item_dict[\"media\"]:\n",
        "                                if isinstance(media_sub_item_dict, dict):\n",
        "                                    _apply_fix(media_sub_item_dict, \"title\")\n",
        "                        else: # It's a direct media item in the list\n",
        "                            _apply_fix(item_dict, \"title\")\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è Structure of {filename_basename} not as expected (missing '{top_level_key}' or not a dict). Skipping detailed fix.\")\n",
        "\n",
        "\n",
        "        elif filename_basename == \"posts_1.json\":\n",
        "            if isinstance(loaded_data, list): # posts_1.json is a list of post objects\n",
        "                for post_item_dict in loaded_data:\n",
        "                    if isinstance(post_item_dict, dict):\n",
        "                        _apply_fix(post_item_dict, \"title\") # Overall post caption/title\n",
        "\n",
        "                        media_list_in_post = post_item_dict.get(\"media\", [])\n",
        "                        if isinstance(media_list_in_post, list):\n",
        "                            for media_item_dict in media_list_in_post:\n",
        "                                if isinstance(media_item_dict, dict):\n",
        "                                    _apply_fix(media_item_dict, \"title\") # Caption for individual media\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è Structure of {filename_basename} not as expected (not a list). Skipping detailed fix.\")\n",
        "\n",
        "        # Add handlers for other specific JSON files here if needed\n",
        "        # else:\n",
        "        #     print(f\"    ‚ÑπÔ∏è No specific ftfy fixing logic defined for {filename_basename}. Text fields might not be fixed.\")\n",
        "\n",
        "        return data_was_changed_locally, titles_fixed_count_locally\n",
        "\n",
        "    # Iterate through all files in the target directory\n",
        "    for filename in os.listdir(TARGET_MEDIA_DIR):\n",
        "        if filename.lower().endswith('.json'):\n",
        "            total_files_processed += 1\n",
        "            current_json_path = os.path.join(TARGET_MEDIA_DIR, filename)\n",
        "            print(f\"\\n--- Processing: {filename} ---\")\n",
        "\n",
        "            # 2. Backup original file\n",
        "            backup_json_path = os.path.join(TARGET_MEDIA_DIR, f\"{os.path.splitext(filename)[0]}-original.json\")\n",
        "            if not os.path.exists(backup_json_path):\n",
        "                try:\n",
        "                    shutil.copy2(current_json_path, backup_json_path)\n",
        "                    print(f\"  üíæ Original backed up to: {os.path.basename(backup_json_path)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è WARNING: Could not create backup of {filename}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ‚ÑπÔ∏è Backup '{os.path.basename(backup_json_path)}' already exists. Not overwriting.\")\n",
        "\n",
        "            # 3. Load, Fix, and Save the current JSON file\n",
        "            try:\n",
        "                print(f\"  L_oading {filename} for fixing...\")\n",
        "                with open(current_json_path, 'r', encoding='utf-8') as f:\n",
        "                    json_content = json.load(f)\n",
        "                print(f\"    {filename} loaded successfully.\")\n",
        "\n",
        "                # Apply ftfy fixes based on file type\n",
        "                data_actually_changed_in_file, titles_fixed_in_file = fix_text_fields_in_data(json_content, filename)\n",
        "\n",
        "                if titles_fixed_in_file > 0:\n",
        "                    print(f\"    üîß Titles/text fields fixed by ftfy in {filename}: {titles_fixed_in_file}\")\n",
        "                    total_titles_fixed_overall += titles_fixed_in_file\n",
        "\n",
        "                if data_actually_changed_in_file:\n",
        "                    total_files_changed_by_ftfy +=1\n",
        "                    print(f\"  S_aving modified {filename} back to disk...\")\n",
        "                    try:\n",
        "                        with open(current_json_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump(json_content, f, ensure_ascii=False, indent=2)\n",
        "                        print(f\"    ‚úÖ Successfully saved fixed {filename}.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ùå ERROR saving modified {filename}: {e}\")\n",
        "                elif titles_fixed_in_file > 0 and not data_actually_changed_in_file : # Should not happen if titles_fixed_in_file > 0\n",
        "                    print(f\"    ‚ÑπÔ∏è ftfy processed text in {filename} but resulted in no net change. File not rewritten.\")\n",
        "                else: # No titles fixed or no changes made\n",
        "                    print(f\"    ‚ÑπÔ∏è No changes made by ftfy to {filename}. File not rewritten.\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"  ‚ùå ERROR: Could not decode JSON from {filename}. The file might be corrupted. Skipping this file.\")\n",
        "            except FileNotFoundError:\n",
        "                 print(f\"  ‚ùå ERROR: File {filename} not found during processing loop (should not happen if listed). Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå An unexpected error occurred while processing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Summary of JSON fixing process ---\")\n",
        "    print(f\"Total .json files found and processed: {total_files_processed}\")\n",
        "    print(f\"Total files modified by ftfy and resaved: {total_files_changed_by_ftfy}\")\n",
        "    print(f\"Total text fields fixed across all files: {total_titles_fixed_overall}\")\n",
        "    print(\"--- Finished JSON fixing process for all media files ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Q8DZoU9kmu",
        "outputId": "36763840-b363-4275-a0ee-c2e8e0e51078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting JSON fixing process for all media files ---\n",
            "‚úÖ Found media directory: /content/extracted_data/your_instagram_activity/media\n",
            "\n",
            "--- Processing: posts_1.json ---\n",
            "  üíæ Original backed up to: posts_1-original.json\n",
            "  L_oading posts_1.json for fixing...\n",
            "    posts_1.json loaded successfully.\n",
            "    üîß Titles/text fields fixed by ftfy in posts_1.json: 225\n",
            "  S_aving modified posts_1.json back to disk...\n",
            "    ‚úÖ Successfully saved fixed posts_1.json.\n",
            "\n",
            "--- Processing: stories.json ---\n",
            "  üíæ Original backed up to: stories-original.json\n",
            "  L_oading stories.json for fixing...\n",
            "    stories.json loaded successfully.\n",
            "    üîß Titles/text fields fixed by ftfy in stories.json: 23\n",
            "  S_aving modified stories.json back to disk...\n",
            "    ‚úÖ Successfully saved fixed stories.json.\n",
            "\n",
            "--- Processing: reels.json ---\n",
            "  üíæ Original backed up to: reels-original.json\n",
            "  L_oading reels.json for fixing...\n",
            "    reels.json loaded successfully.\n",
            "    üîß Titles/text fields fixed by ftfy in reels.json: 14\n",
            "  S_aving modified reels.json back to disk...\n",
            "    ‚úÖ Successfully saved fixed reels.json.\n",
            "\n",
            "--- Processing: recently_deleted_content.json ---\n",
            "  üíæ Original backed up to: recently_deleted_content-original.json\n",
            "  L_oading recently_deleted_content.json for fixing...\n",
            "    recently_deleted_content.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to recently_deleted_content.json. File not rewritten.\n",
            "\n",
            "--- Processing: profile_photos.json ---\n",
            "  üíæ Original backed up to: profile_photos-original.json\n",
            "  L_oading profile_photos.json for fixing...\n",
            "    profile_photos.json loaded successfully.\n",
            "    ‚ÑπÔ∏è No changes made by ftfy to profile_photos.json. File not rewritten.\n",
            "\n",
            "--- Summary of JSON fixing process ---\n",
            "Total .json files found and processed: 5\n",
            "Total files modified by ftfy and resaved: 3\n",
            "Total text fields fixed across all files: 262\n",
            "--- Finished JSON fixing process for all media files ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from shutil import copy2\n",
        "import re # For hashtag extraction\n",
        "\n",
        "# Ensure MEDIA_DIR is defined, as it's used in subsequent cells\n",
        "MEDIA_DIR = \"/content/extracted_data\" # This should be consistent with the extraction path\n",
        "\n",
        "media_type = 'reels' # We are focusing on reels\n",
        "output_dir = os.path.join(MEDIA_DIR, f'{media_type}_export_dc_format') # New output dir name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Excel and ZIP file paths\n",
        "excel_filename = f'{media_type}_metadata_dc_format.xlsx'\n",
        "excel_path = os.path.join(output_dir, excel_filename)\n",
        "readme_path = os.path.join(output_dir, 'README.txt')\n",
        "zip_path = os.path.join(output_dir, f'{media_type}_package_dc_format.zip') # New zip name\n",
        "\n",
        "# Attempt to get username_value from global scope (set in a previous cell)\n",
        "instagram_handle = globals().get('username_value', \"unknown_user\")\n",
        "print(f\"Using Instagram handle: {instagram_handle}\")\n",
        "\n",
        "# --- Helper function to extract hashtags ---\n",
        "def extract_hashtags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return \", \".join(hashtags)\n",
        "\n",
        "# --- Load Reels JSON Data ---\n",
        "instagram_media_data = {}\n",
        "try:\n",
        "    reels_json_path = os.path.join(MEDIA_DIR, 'your_instagram_activity', 'media', 'reels.json')\n",
        "    if os.path.exists(reels_json_path):\n",
        "         with open(reels_json_path, 'r', encoding='utf-8') as f:\n",
        "            reels_full_data = json.load(f)\n",
        "            if isinstance(reels_full_data, dict) and \"ig_reels_media\" in reels_full_data:\n",
        "                 flat_list = []\n",
        "                 for group in reels_full_data[\"ig_reels_media\"]:\n",
        "                     if isinstance(group, dict) and \"media\" in group:\n",
        "                         flat_list.extend(group[\"media\"])\n",
        "                     else:\n",
        "                         flat_list.append(group)\n",
        "                 instagram_media_data['reels'] = flat_list # Assign to 'reels' key\n",
        "            else:\n",
        "                print(\"Warning: Reels JSON structure unexpected or 'ig_reels_media' key missing.\")\n",
        "                instagram_media_data['reels'] = []\n",
        "    else:\n",
        "        print(f\"Reels JSON file not found at: {reels_json_path}\")\n",
        "        instagram_media_data['reels'] = []\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Reels JSON for export: {e}\")\n",
        "    instagram_media_data['reels'] = []\n",
        "\n",
        "# --- Process Reels Data ---\n",
        "reels_data_to_process = instagram_media_data.get('reels') # Get data from 'reels' key\n",
        "\n",
        "if not reels_data_to_process:\n",
        "    print(\"No Reels data loaded or found for export.\")\n",
        "else:\n",
        "    print(f\"Preparing to export metadata for {len(reels_data_to_process)} Reels in Digital Commons format...\")\n",
        "\n",
        "    excel_data_rows = []\n",
        "\n",
        "    # Define the new column order for the Excel sheet\n",
        "    column_names = [\n",
        "        'title', 'fulltext_url', 'additional_files', 'keywords', 'abstract',\n",
        "        'author1_fname', 'author1_mname', 'author1_lname', 'author1_suffix',\n",
        "        'author1_email', 'author1_institution', 'author1_is_corporate',\n",
        "        'author2_fname', 'author2_mname', 'author2_lname', 'author2_suffix',\n",
        "        'author2_email', 'author2_institution', 'author2_is_corporate',\n",
        "        'author3_fname', 'author3_mname', 'author3_lname', 'author3_suffix',\n",
        "        'author3_email', 'author3_institution', 'author3_is_corporate',\n",
        "        'author4_fname', 'author4_mname', 'author4_lname', 'author4_suffix',\n",
        "        'author4_email', 'author4_institution', 'author4_is_corporate',\n",
        "        'disciplines', 'instagram_username', 'document_type'\n",
        "    ]\n",
        "\n",
        "    copied_files_for_zip = []\n",
        "\n",
        "    for i, item in enumerate(reels_data_to_process):\n",
        "        original_reel_uri = item.get('uri')\n",
        "        if not original_reel_uri:\n",
        "            print(f\"Warning: Reel item {i} has no URI, skipping.\")\n",
        "            continue\n",
        "\n",
        "        media_path = os.path.join(MEDIA_DIR, original_reel_uri)\n",
        "        if not os.path.exists(media_path):\n",
        "            print(f\"Warning: Media file not found - {media_path}, skipping reel item.\")\n",
        "            continue\n",
        "\n",
        "        # --- Date and Filename (Reel Video) ---\n",
        "        timestamp = item.get('creation_timestamp')\n",
        "        date_obj = datetime.fromtimestamp(timestamp) if timestamp else datetime.now()\n",
        "        date_str_for_title = date_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "        ext = os.path.splitext(original_reel_uri)[-1]\n",
        "        handle_for_filename = instagram_handle if instagram_handle else \"unknown_user\"\n",
        "        original_file_basename = os.path.splitext(os.path.basename(original_reel_uri))[0]\n",
        "        sanitized_original_basename = ''.join(c if c.isalnum() else '_' for c in original_file_basename)\n",
        "\n",
        "        # This is the reel video filename, goes into 'fulltext_url'\n",
        "        reel_export_filename = f\"instagram_{handle_for_filename}_reel_{date_str_for_title}_{i+1}_{sanitized_original_basename}{ext}\"\n",
        "        reel_export_path = os.path.join(output_dir, reel_export_filename)\n",
        "\n",
        "        try:\n",
        "            copy2(media_path, reel_export_path)\n",
        "            copied_files_for_zip.append(reel_export_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying media file {media_path} to {reel_export_path}: {e}. Skipping this reel.\")\n",
        "            continue\n",
        "\n",
        "        # --- Abstract and Keywords ---\n",
        "        original_caption = item.get('title', '') # This is the Instagram caption\n",
        "        keywords_str = extract_hashtags(original_caption)\n",
        "\n",
        "        # --- SRT File (Additional File) ---\n",
        "        srt_export_filename = '' # Initialize to empty\n",
        "        original_srt_uri = ''\n",
        "\n",
        "        try:\n",
        "            subtitles_data = item.get('media_metadata', {}).get('video_metadata', {}).get('subtitles', {})\n",
        "            original_srt_uri = subtitles_data.get('uri', '')\n",
        "\n",
        "            if original_srt_uri:\n",
        "                srt_original_full_path = os.path.join(MEDIA_DIR, original_srt_uri)\n",
        "                if os.path.exists(srt_original_full_path):\n",
        "                    base_export_videoname, _ = os.path.splitext(reel_export_filename)\n",
        "                    srt_export_filename = base_export_videoname + \".srt\" # This goes into 'additional_files'\n",
        "                    srt_export_path = os.path.join(output_dir, srt_export_filename)\n",
        "\n",
        "                    copy2(srt_original_full_path, srt_export_path)\n",
        "                    copied_files_for_zip.append(srt_export_path)\n",
        "                else:\n",
        "                    # srt_export_filename remains empty if source SRT not found\n",
        "                    pass\n",
        "        except (KeyError, TypeError):\n",
        "            # srt_export_filename remains empty\n",
        "            pass\n",
        "\n",
        "        # --- Construct new title ---\n",
        "        dc_title = f\"Instagram {handle_for_filename} {date_str_for_title}\"\n",
        "\n",
        "        # --- Prepare row data for Excel ---\n",
        "        row = {\n",
        "            'title': dc_title,\n",
        "            'fulltext_url': reel_export_filename,\n",
        "            'additional_files': srt_export_filename, # Will be empty if no SRT\n",
        "            'keywords': keywords_str,\n",
        "            'abstract': original_caption,\n",
        "            'author1_fname': '', 'author1_mname': '', 'author1_lname': '', 'author1_suffix': '',\n",
        "            'author1_email': '', 'author1_institution': '', 'author1_is_corporate': '',\n",
        "            'author2_fname': '', 'author2_mname': '', 'author2_lname': '', 'author2_suffix': '',\n",
        "            'author2_email': '', 'author2_institution': '', 'author2_is_corporate': '',\n",
        "            'author3_fname': '', 'author3_mname': '', 'author3_lname': '', 'author3_suffix': '',\n",
        "            'author3_email': '', 'author3_institution': '', 'author3_is_corporate': '',\n",
        "            'author4_fname': '', 'author4_mname': '', 'author4_lname': '', 'author4_suffix': '',\n",
        "            'author4_email': '', 'author4_institution': '', 'author4_is_corporate': '',\n",
        "            'disciplines': '',\n",
        "            'instagram_username': instagram_handle,\n",
        "            'document_type': 'Instagram Reel'\n",
        "        }\n",
        "        excel_data_rows.append(row)\n",
        "\n",
        "    # --- Create DataFrame and save to Excel ---\n",
        "    if excel_data_rows:\n",
        "        df = pd.DataFrame(excel_data_rows, columns=column_names)\n",
        "        try:\n",
        "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"Metadata written to Excel: {excel_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Excel file {excel_path}: {e}\")\n",
        "            print(\"Make sure 'openpyxl' library is installed (pip install openpyxl).\")\n",
        "    else:\n",
        "        print(\"No data to write to Excel.\")\n",
        "\n",
        "    # --- Write README file (Updated) ---\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"\"\"Instagram Reels Export Package (Digital Commons Format)\n",
        "=====================================================\n",
        "\n",
        "Handle: @{instagram_handle}\n",
        "Exported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "This package contains:\n",
        "- Exported Reel video files.\n",
        "- Associated .srt subtitle files (if they existed in the archive and were found), renamed to match their corresponding video files.\n",
        "- An Excel file ({excel_filename}) with metadata for each reel, formatted for potential Digital Commons import.\n",
        "- This README.txt file.\n",
        "\n",
        "Fields in the Excel file ({excel_filename}):\n",
        "- title: A generated title in the format \"Instagram [username] [YYYY-MM-DD]\".\n",
        "- fulltext_url: The filename of the exported reel video file in this package. This file should be uploaded as the primary file.\n",
        "- additional_files: The filename of the exported .srt subtitle file (if available) in this package. This should be uploaded as an additional file.\n",
        "- keywords: Comma-separated hashtags extracted from the reel's original caption.\n",
        "- abstract: The original caption/text of the Instagram reel.\n",
        "- author1_fname to author4_is_corporate: Fields for author information (currently left blank).\n",
        "- disciplines: Field for academic disciplines (currently left blank).\n",
        "- instagram_username: The Instagram handle from which the reel originated.\n",
        "- document_type: Set to \"Instagram Reel\".\n",
        "\n",
        "Generated by Instagram archive processing script.\n",
        "\"\"\")\n",
        "    print(f\"README file written to: {readme_path}\")\n",
        "\n",
        "    # --- Zip everything up ---\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        if os.path.exists(excel_path):\n",
        "            zipf.write(excel_path, arcname=os.path.basename(excel_path))\n",
        "        zipf.write(readme_path, arcname='README.txt')\n",
        "        for file_to_zip in copied_files_for_zip: # Includes videos and SRTs\n",
        "            if os.path.exists(file_to_zip):\n",
        "                zipf.write(file_to_zip, arcname=os.path.basename(file_to_zip))\n",
        "            else:\n",
        "                print(f\"Warning: File {file_to_zip} not found for zipping.\")\n",
        "    print(f\"ZIP archive created: {zip_path}\")\n",
        "\n",
        "# The last cell (for downloading) will need to be updated to point to this new zip_path\n",
        "# e.g., source_zip_path = \"/content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\"\n",
        "# and target_zip_filename = \"reels_dc_format.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS-P7cYjp5xh",
        "outputId": "77955c80-5951-448b-86b8-6faa9975cef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Instagram handle: umsllibraries\n",
            "Preparing to export metadata for 25 Reels in Digital Commons format...\n",
            "Metadata written to Excel: /content/extracted_data/reels_export_dc_format/reels_metadata_dc_format.xlsx\n",
            "README file written to: /content/extracted_data/reels_export_dc_format/README.txt\n",
            "ZIP archive created: /content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files # Make sure this is imported\n",
        "\n",
        "# --- Assuming your previous script has run and created the source ZIP ---\n",
        "# Define the source and target paths\n",
        "source_zip_path = \"/content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\"\n",
        "target_base_dir = \"/content/batchup\" # The directory where you want the new ZIP\n",
        "target_zip_filename = \"reels.zip\"     # The desired name for the ZIP in the target directory\n",
        "target_zip_path = os.path.join(target_base_dir, target_zip_filename)\n",
        "\n",
        "# 1. Check if the source ZIP file exists\n",
        "if not os.path.exists(source_zip_path):\n",
        "    print(f\"‚ùå ERROR: Source ZIP file not found at: {source_zip_path}\")\n",
        "    print(\"Please ensure the previous steps to create the ZIP were successful.\")\n",
        "else:\n",
        "    print(f\"‚úîÔ∏è Source ZIP found: {source_zip_path}\")\n",
        "\n",
        "    # 2. Ensure the target directory exists, create it if not\n",
        "    os.makedirs(target_base_dir, exist_ok=True)\n",
        "    print(f\"‚úîÔ∏è Ensured target directory exists: {target_base_dir}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Copy the file\n",
        "        shutil.copy2(source_zip_path, target_zip_path) # copy2 preserves metadata\n",
        "        print(f\"‚úÖ Successfully copied '{os.path.basename(source_zip_path)}' to '{target_zip_path}'\")\n",
        "\n",
        "        # 4. Offer a download link for the new file\n",
        "        print(f\"\\n‚¨áÔ∏è Click the link below to download '{target_zip_filename}':\")\n",
        "        # Note: files.download() directly initiates the download in the browser\n",
        "        # It doesn't print a clickable link in the classic HTML sense in the output cell,\n",
        "        # but Colab's UI will typically show a download prompt or progress.\n",
        "        files.download(target_zip_path)\n",
        "        print(f\"(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\")\n",
        "        print(f\"The file is located at: {target_zip_path} in the Colab environment.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during copy or download: {e}\")\n",
        "\n",
        "# Example of how you might integrate this at the end of your existing script:\n",
        "# ... (your existing script that creates /content/extracted_data/reels_export/reels_package.zip) ...\n",
        "# print(f\"ZIP archive created: {zip_path}\") # zip_path from your previous script would be source_zip_path here\n",
        "\n",
        "# --- Add the copy and download logic here ---\n",
        "# (The code block above would go here, making sure source_zip_path matches\n",
        "# the zip_path variable from your previous script part if you used a variable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "uAKOItDGze-_",
        "outputId": "eeb4bc04-9f76-46e4-a27e-f04b3d8d3d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Source ZIP found: /content/extracted_data/reels_export_dc_format/reels_package_dc_format.zip\n",
            "‚úîÔ∏è Ensured target directory exists: /content/batchup\n",
            "‚úÖ Successfully copied 'reels_package_dc_format.zip' to '/content/batchup/reels.zip'\n",
            "\n",
            "‚¨áÔ∏è Click the link below to download 'reels.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_05623da1-b272-4f5c-b9de-28bd96fa3106\", \"reels.zip\", 154620002)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\n",
            "The file is located at: /content/batchup/reels.zip in the Colab environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files, drive # files is used for download in a later cell\n",
        "import zipfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from shutil import copy2\n",
        "import re # For hashtag extraction\n",
        "\n",
        "# Ensure MEDIA_DIR is defined, as it's used in subsequent cells\n",
        "MEDIA_DIR = \"/content/extracted_data\" # This should be consistent with the extraction path\n",
        "\n",
        "media_type = 'stories' # We are focusing on stories\n",
        "output_dir = os.path.join(MEDIA_DIR, f'{media_type}_export_dc_format') # New output dir name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Created output directory for stories: {output_dir}\")\n",
        "\n",
        "# Excel and ZIP file paths\n",
        "excel_filename = f'{media_type}_metadata_dc_format.xlsx'\n",
        "excel_path = os.path.join(output_dir, excel_filename)\n",
        "readme_path = os.path.join(output_dir, 'README.txt')\n",
        "zip_package_filename = f'{media_type}_package_dc_format.zip'\n",
        "zip_path = os.path.join(output_dir, zip_package_filename)\n",
        "\n",
        "instagram_handle = globals().get('username_value', \"unknown_user\")\n",
        "print(f\"Using Instagram handle: {instagram_handle}\")\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return \", \".join(hashtags)\n",
        "\n",
        "# --- Load Stories JSON Data ---\n",
        "instagram_media_data = {}\n",
        "total_stories_in_json = 0\n",
        "try:\n",
        "    stories_json_path = os.path.join(MEDIA_DIR, 'your_instagram_activity', 'media', 'stories.json')\n",
        "    if os.path.exists(stories_json_path):\n",
        "        with open(stories_json_path, 'r', encoding='utf-8') as f:\n",
        "            stories_full_data = json.load(f)\n",
        "            if isinstance(stories_full_data, dict) and \"ig_stories\" in stories_full_data and isinstance(stories_full_data[\"ig_stories\"], list):\n",
        "                # Sort by creation_timestamp to group items from the same \"post time\" consecutively\n",
        "                # Handle items with no timestamp by placing them at the end (or beginning, consistently)\n",
        "                instagram_media_data['stories'] = sorted(\n",
        "                    stories_full_data[\"ig_stories\"],\n",
        "                    key=lambda x: x.get('creation_timestamp', float('inf')) # Sort None/missing timestamps last\n",
        "                )\n",
        "                total_stories_in_json = len(stories_full_data[\"ig_stories\"])\n",
        "            elif isinstance(stories_full_data, list):\n",
        "                print(\"Assuming the JSON root is the list of stories (older format or direct list). Sorting by timestamp.\")\n",
        "                instagram_media_data['stories'] = sorted(\n",
        "                    stories_full_data,\n",
        "                    key=lambda x: x.get('creation_timestamp', float('inf'))\n",
        "                )\n",
        "                total_stories_in_json = len(stories_full_data)\n",
        "            else:\n",
        "                print(f\"Warning: Stories JSON structure unexpected. Expected dict with 'ig_stories' list or a direct list. Found: {type(stories_full_data)}\")\n",
        "                instagram_media_data['stories'] = []\n",
        "    else:\n",
        "        print(f\"Stories JSON file not found at: {stories_json_path}\")\n",
        "        instagram_media_data['stories'] = []\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or sorting Stories JSON for export: {e}\")\n",
        "    instagram_media_data['stories'] = []\n",
        "\n",
        "# --- Process Stories Data ---\n",
        "stories_data_to_process = instagram_media_data.get('stories')\n",
        "skipped_absolute_uri_count = 0\n",
        "skipped_missing_local_file_count = 0\n",
        "skipped_no_uri_count = 0\n",
        "skipped_no_extension_count = 0\n",
        "processed_item_excel_count = 0 # Counter for items written to Excel\n",
        "\n",
        "# For handling repeated metadata for same-timestamp posts\n",
        "last_processed_group_timestamp = None\n",
        "shared_group_metadata = {\n",
        "    'title': '',\n",
        "    'abstract': '',\n",
        "    'keywords': ''\n",
        "}\n",
        "\n",
        "if not stories_data_to_process:\n",
        "    print(\"No Stories data loaded or found for export.\")\n",
        "else:\n",
        "    print(f\"Found {total_stories_in_json} story entries in JSON. Preparing to export metadata for locally available items...\")\n",
        "\n",
        "    excel_data_rows = []\n",
        "    copied_files_for_zip = []\n",
        "\n",
        "    column_names = [\n",
        "        'title', 'fulltext_url', 'additional_files', 'keywords', 'abstract',\n",
        "        'author1_fname', 'author1_mname', 'author1_lname', 'author1_suffix',\n",
        "        'author1_email', 'author1_institution', 'author1_is_corporate',\n",
        "        'disciplines', 'instagram_username', 'document_type'\n",
        "    ]\n",
        "\n",
        "    for i, item in enumerate(stories_data_to_process): # i is index after sorting\n",
        "        original_story_uri = item.get('uri')\n",
        "\n",
        "        if not original_story_uri:\n",
        "            skipped_no_uri_count += 1\n",
        "            continue\n",
        "        if original_story_uri.startswith('http://') or original_story_uri.startswith('https://'):\n",
        "            skipped_absolute_uri_count +=1\n",
        "            continue\n",
        "\n",
        "        media_path = os.path.join(MEDIA_DIR, original_story_uri)\n",
        "        if not os.path.exists(media_path):\n",
        "            skipped_missing_local_file_count += 1\n",
        "            continue\n",
        "\n",
        "        current_item_timestamp = item.get('creation_timestamp')\n",
        "\n",
        "        date_obj = datetime.fromtimestamp(current_item_timestamp) if current_item_timestamp else datetime.now()\n",
        "        date_str_for_filename_and_title = date_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "        _, ext = os.path.splitext(original_story_uri)\n",
        "        if not ext:\n",
        "            if 'video_metadata' in item.get('media_metadata', {}): ext = '.mp4'\n",
        "            elif 'photo_metadata' in item.get('media_metadata', {}): ext = '.jpg'\n",
        "            else:\n",
        "                skipped_no_extension_count +=1\n",
        "                continue\n",
        "\n",
        "        handle_for_filename = instagram_handle.replace(\"@\", \"\") if instagram_handle else \"unknown_user\"\n",
        "        original_file_basename = os.path.splitext(os.path.basename(original_story_uri))[0]\n",
        "        sanitized_original_basename = ''.join(c if c.isalnum() else '_' for c in original_file_basename).strip('_')\n",
        "        if not sanitized_original_basename:\n",
        "             sanitized_original_basename = f\"media_item_{processed_item_excel_count + 1}\"\n",
        "\n",
        "        # Filename must be unique for each distinct media file.\n",
        "        # Using processed_item_excel_count ensures unique numbering for exported files.\n",
        "        story_export_filename = f\"instagram_{handle_for_filename}_story_{date_str_for_filename_and_title}_{processed_item_excel_count + 1}_{sanitized_original_basename}{ext}\"\n",
        "        story_export_path = os.path.join(output_dir, story_export_filename)\n",
        "\n",
        "        try:\n",
        "            copy2(media_path, story_export_path)\n",
        "            copied_files_for_zip.append(story_export_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying media file {media_path} to {story_export_path}: {e}. Skipping this story.\")\n",
        "            skipped_missing_local_file_count += 1\n",
        "            continue\n",
        "\n",
        "        # --- Handle grouped metadata based on timestamp ---\n",
        "        if current_item_timestamp != last_processed_group_timestamp or last_processed_group_timestamp is None:\n",
        "            # This is the first item of a new timestamp group (or the very first item overall)\n",
        "            shared_group_metadata['abstract'] = item.get('title', '') # Story's text/caption\n",
        "            shared_group_metadata['keywords'] = extract_hashtags(shared_group_metadata['abstract'])\n",
        "            shared_group_metadata['title'] = f\"Instagram Story by {instagram_handle} - {date_str_for_filename_and_title}\"\n",
        "            last_processed_group_timestamp = current_item_timestamp\n",
        "\n",
        "        # All items in the same timestamp group will use these:\n",
        "        dc_title_to_use = shared_group_metadata['title']\n",
        "        abstract_to_use = shared_group_metadata['abstract']\n",
        "        keywords_to_use = shared_group_metadata['keywords']\n",
        "\n",
        "        processed_item_excel_count += 1\n",
        "\n",
        "        row = {\n",
        "            'title': dc_title_to_use,\n",
        "            'fulltext_url': story_export_filename, # Unique per file\n",
        "            'additional_files': '',\n",
        "            'keywords': keywords_to_use,\n",
        "            'abstract': abstract_to_use,\n",
        "            'author1_fname': '', 'author1_mname': '', 'author1_lname': '', 'author1_suffix': '',\n",
        "            'author1_email': '', 'author1_institution': '', 'author1_is_corporate': False,\n",
        "            'disciplines': '',\n",
        "            'instagram_username': instagram_handle,\n",
        "            'document_type': 'Instagram Story'\n",
        "        }\n",
        "        excel_data_rows.append(row)\n",
        "\n",
        "    print(f\"\\n--- Processing Summary ---\")\n",
        "    print(f\"Total story entries in JSON: {total_stories_in_json}\")\n",
        "    print(f\"Successfully processed and included in package (rows in Excel): {processed_item_excel_count}\")\n",
        "    print(f\"Stories skipped due to being a web link (absolute URI): {skipped_absolute_uri_count}\")\n",
        "    print(f\"Stories skipped because local media file was missing: {skipped_missing_local_file_count}\")\n",
        "    print(f\"Stories skipped because JSON entry had no URI: {skipped_no_uri_count}\")\n",
        "    print(f\"Stories skipped because media type/extension couldn't be determined: {skipped_no_extension_count}\")\n",
        "    print(f\"--------------------------\\n\")\n",
        "\n",
        "    if excel_data_rows:\n",
        "        df = pd.DataFrame(excel_data_rows, columns=column_names)\n",
        "        try:\n",
        "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"Metadata for {len(excel_data_rows)} locally available stories written to Excel: {excel_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Excel file {excel_path}: {e}\")\n",
        "            print(\"Make sure 'openpyxl' library is installed (e.g., !pip install openpyxl).\")\n",
        "    else:\n",
        "        print(\"No locally available story data processed to write to Excel.\")\n",
        "\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"\"\"Instagram Stories Export Package (Digital Commons Format)\n",
        "========================================================\n",
        "\n",
        "Handle: @{instagram_handle}\n",
        "Exported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "This package contains:\n",
        "- Exported Story media files (images or videos) that were found locally within your downloaded Instagram archive.\n",
        "- An Excel file ({excel_filename}) with metadata for these locally available stories, formatted for potential Digital Commons import.\n",
        "- This README.txt file.\n",
        "\n",
        "Important Note on Metadata for Grouped Stories:\n",
        "-----------------------------------------------\n",
        "If multiple story items were posted at the exact same time (same `creation_timestamp`),\n",
        "this script groups them. For such groups:\n",
        "- The 'title', 'abstract', and 'keywords' metadata fields in the Excel sheet will be\n",
        "  identical for all items within that same-timestamp group. This metadata is taken\n",
        "  from the first story item encountered within that group.\n",
        "- Each story media file itself (referenced in 'fulltext_url') remains unique and is\n",
        "  individually packaged.\n",
        "\n",
        "Important Note on Potentially Missing Story Items:\n",
        "-------------------------------------------------\n",
        "The number of stories included in this package might be less than the total number of\n",
        "story entries in your `stories.json` file. This is because:\n",
        "1. Some story entries in `stories.json` may reference media using a web link\n",
        "   (e.g., starting with \"http://\"). These point to files on Instagram's servers,\n",
        "   not files included locally in your archive.\n",
        "2. A story entry might list a local file path, but that file could be missing from\n",
        "   your specific archive download or the script couldn't determine its type.\n",
        "3. Some JSON entries might be incomplete (e.g., missing a URI).\n",
        "\n",
        "This script packages *only* media files physically present and accessible within your\n",
        "downloaded Instagram archive. It does *not* download files from web links.\n",
        "\n",
        "Summary from this export run:\n",
        "- Total story entries originally in stories.json: {total_stories_in_json}\n",
        "- Stories successfully processed and included in this package: {processed_item_excel_count}\n",
        "- Stories skipped (media was a web link): {skipped_absolute_uri_count}\n",
        "- Stories skipped (local media file missing): {skipped_missing_local_file_count}\n",
        "- Stories skipped (JSON entry had no URI): {skipped_no_uri_count}\n",
        "- Stories skipped (media type/extension undetermined): {skipped_no_extension_count}\n",
        "\n",
        "Fields in the Excel file ({excel_filename}):\n",
        "- title: A generated title. For stories posted at the same time, this title will be repeated.\n",
        "- fulltext_url: The unique filename of the exported story media file.\n",
        "- additional_files: Typically empty for stories.\n",
        "- keywords: Hashtags from the story's text. Repeated for same-timestamp groups.\n",
        "- abstract: The story's text. Repeated for same-timestamp groups.\n",
        "- (Author fields): For author information.\n",
        "- disciplines: For academic disciplines.\n",
        "- instagram_username: The Instagram handle.\n",
        "- document_type: \"Instagram Story\".\n",
        "\n",
        "Generated by Instagram archive processing script.\n",
        "\"\"\")\n",
        "    print(f\"README file written to: {readme_path}\")\n",
        "\n",
        "    if copied_files_for_zip or (os.path.exists(excel_path) and excel_data_rows):\n",
        "        print(f\"Creating ZIP archive: {zip_path}\")\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            if os.path.exists(excel_path) and excel_data_rows:\n",
        "                zipf.write(excel_path, arcname=os.path.basename(excel_path))\n",
        "                print(f\"Added {os.path.basename(excel_path)} to ZIP.\")\n",
        "            elif not excel_data_rows: print(f\"Excel file {excel_filename} not added (no data).\")\n",
        "            elif not os.path.exists(excel_path): print(f\"Excel file {excel_path} not found.\")\n",
        "\n",
        "            if os.path.exists(readme_path):\n",
        "                zipf.write(readme_path, arcname='README.txt')\n",
        "                print(f\"Added README.txt to ZIP.\")\n",
        "            else: print(f\"README.txt not found at {readme_path}.\")\n",
        "\n",
        "            successfully_zipped_media_count = 0\n",
        "            for file_to_zip in copied_files_for_zip:\n",
        "                if os.path.exists(file_to_zip):\n",
        "                    zipf.write(file_to_zip, arcname=os.path.basename(file_to_zip))\n",
        "                    successfully_zipped_media_count += 1\n",
        "                else:\n",
        "                    print(f\"Warning: Copied media file {file_to_zip} not found during zipping.\")\n",
        "            print(f\"Added {successfully_zipped_media_count} media files to ZIP.\")\n",
        "        print(f\"ZIP archive '{zip_package_filename}' created successfully in '{output_dir}'.\")\n",
        "    else:\n",
        "        print(f\"No files processed/copied for stories. ZIP archive '{zip_package_filename}' not created.\")\n",
        "\n",
        "print(f\"\\n--- Story Export Process Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXTUR6yUiDjk",
        "outputId": "d7af660e-f65a-4b04-880d-4316b1344e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directory for stories: /content/extracted_data/stories_export_dc_format\n",
            "Using Instagram handle: umsllibraries\n",
            "Found 485 story entries in JSON. Preparing to export metadata for locally available items...\n",
            "\n",
            "--- Processing Summary ---\n",
            "Total story entries in JSON: 485\n",
            "Successfully processed and included in package (rows in Excel): 450\n",
            "Stories skipped due to being a web link (absolute URI): 35\n",
            "Stories skipped because local media file was missing: 0\n",
            "Stories skipped because JSON entry had no URI: 0\n",
            "Stories skipped because media type/extension couldn't be determined: 0\n",
            "--------------------------\n",
            "\n",
            "Metadata for 450 locally available stories written to Excel: /content/extracted_data/stories_export_dc_format/stories_metadata_dc_format.xlsx\n",
            "README file written to: /content/extracted_data/stories_export_dc_format/README.txt\n",
            "Creating ZIP archive: /content/extracted_data/stories_export_dc_format/stories_package_dc_format.zip\n",
            "Added stories_metadata_dc_format.xlsx to ZIP.\n",
            "Added README.txt to ZIP.\n",
            "Added 450 media files to ZIP.\n",
            "ZIP archive 'stories_package_dc_format.zip' created successfully in '/content/extracted_data/stories_export_dc_format'.\n",
            "\n",
            "--- Story Export Process Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Assuming your previous script has run and created the source ZIP ---\n",
        "# Define the source and target paths\n",
        "source_zip_path = \"/content/extracted_data/stories_export_dc_format/stories_package_dc_format.zip\"\n",
        "target_base_dir = \"/content/batchup\" # The directory where you want the new ZIP\n",
        "target_zip_filename = \"stories.zip\"     # The desired name for the ZIP in the target directory\n",
        "target_zip_path = os.path.join(target_base_dir, target_zip_filename)\n",
        "\n",
        "# 1. Check if the source ZIP file exists\n",
        "if not os.path.exists(source_zip_path):\n",
        "    print(f\"‚ùå ERROR: Source ZIP file not found at: {source_zip_path}\")\n",
        "    print(\"Please ensure the previous steps to create the ZIP were successful.\")\n",
        "else:\n",
        "    print(f\"‚úîÔ∏è Source ZIP found: {source_zip_path}\")\n",
        "\n",
        "    # 2. Ensure the target directory exists, create it if not\n",
        "    os.makedirs(target_base_dir, exist_ok=True)\n",
        "    print(f\"‚úîÔ∏è Ensured target directory exists: {target_base_dir}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Copy the file\n",
        "        shutil.copy2(source_zip_path, target_zip_path) # copy2 preserves metadata\n",
        "        print(f\"‚úÖ Successfully copied '{os.path.basename(source_zip_path)}' to '{target_zip_path}'\")\n",
        "\n",
        "        # 4. Offer a download link for the new file\n",
        "        print(f\"\\n‚¨áÔ∏è Click the link below to download '{target_zip_filename}':\")\n",
        "        # Note: files.download() directly initiates the download in the browser\n",
        "        # It doesn't print a clickable link in the classic HTML sense in the output cell,\n",
        "        # but Colab's UI will typically show a download prompt or progress.\n",
        "        files.download(target_zip_path)\n",
        "        print(f\"(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\")\n",
        "        print(f\"The file is located at: {target_zip_path} in the Colab environment.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during copy or download: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "XD1XOqQtiNXY",
        "outputId": "197c52ec-83e3-46ff-eccd-740ddf1e9754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Source ZIP found: /content/extracted_data/stories_export_dc_format/stories_package_dc_format.zip\n",
            "‚úîÔ∏è Ensured target directory exists: /content/batchup\n",
            "‚úÖ Successfully copied 'stories_package_dc_format.zip' to '/content/batchup/stories.zip'\n",
            "\n",
            "‚¨áÔ∏è Click the link below to download 'stories.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14c2aa39-eb33-4caa-abc5-2d8187d96321\", \"stories.zip\", 60380844)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\n",
            "The file is located at: /content/batchup/stories.zip in the Colab environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "# from google.colab import files, drive # files is used for download in a later cell. Keep if this cell is standalone runnable.\n",
        "import zipfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from shutil import copy2\n",
        "import re # For hashtag extraction\n",
        "\n",
        "# Ensure MEDIA_DIR is defined, as it's used in subsequent cells\n",
        "MEDIA_DIR = \"/content/extracted_data\" # This should be consistent with the extraction path\n",
        "\n",
        "media_type = 'posts' # We are focusing on posts\n",
        "output_dir = os.path.join(MEDIA_DIR, f'{media_type}_export_dc_format_individual_media') # New output dir name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Created output directory for posts (individual media items): {output_dir}\")\n",
        "\n",
        "# Excel and ZIP file paths\n",
        "excel_filename = f'{media_type}_metadata_dc_format_individual_media.xlsx'\n",
        "excel_path = os.path.join(output_dir, excel_filename)\n",
        "readme_path = os.path.join(output_dir, 'README.txt')\n",
        "zip_package_filename = f'{media_type}_package_dc_format_individual_media.zip'\n",
        "zip_path = os.path.join(output_dir, zip_package_filename)\n",
        "\n",
        "instagram_handle = globals().get('username_value', \"unknown_user\")\n",
        "print(f\"Using Instagram handle: {instagram_handle}\")\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    return \", \".join(hashtags)\n",
        "\n",
        "# --- Load Posts JSON Data ---\n",
        "instagram_media_data = {}\n",
        "total_posts_in_json = 0\n",
        "try:\n",
        "    posts_json_path = os.path.join(MEDIA_DIR, 'your_instagram_activity', 'media', 'posts_1.json')\n",
        "    if os.path.exists(posts_json_path):\n",
        "        with open(posts_json_path, 'r', encoding='utf-8') as f:\n",
        "            loaded_post_data = json.load(f) # posts_1.json is a list of post objects\n",
        "            if isinstance(loaded_post_data, list):\n",
        "                # Sort posts by creation_timestamp if available, to process chronologically\n",
        "                # Handle items with no timestamp by placing them at the end\n",
        "                instagram_media_data['posts'] = sorted(\n",
        "                    loaded_post_data,\n",
        "                    key=lambda x: x.get('creation_timestamp', x.get('media', [{}])[0].get('creation_timestamp', float('inf')))\n",
        "                                  if isinstance(x.get('media'), list) and x.get('media') else float('inf')\n",
        "                )\n",
        "                total_posts_in_json = len(loaded_post_data)\n",
        "            else:\n",
        "                print(f\"Warning: Posts JSON ({posts_json_path}) was not a list as expected. Found type: {type(loaded_post_data)}\")\n",
        "                instagram_media_data['posts'] = []\n",
        "    else:\n",
        "        print(f\"Posts JSON file not found at: {posts_json_path}\")\n",
        "        instagram_media_data['posts'] = []\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or sorting Posts JSON for export: {e}\")\n",
        "    instagram_media_data['posts'] = []\n",
        "\n",
        "# --- Process Posts Data ---\n",
        "posts_data_to_process = instagram_media_data.get('posts')\n",
        "\n",
        "# Counters for summary\n",
        "processed_media_items_excel_count = 0 # Counts each media item that gets a row\n",
        "skipped_posts_no_media_list = 0\n",
        "skipped_posts_no_timestamp = 0\n",
        "skipped_media_items_no_uri = 0\n",
        "skipped_media_items_absolute_uri = 0\n",
        "skipped_media_items_missing_local_file = 0\n",
        "skipped_media_items_no_extension = 0\n",
        "\n",
        "\n",
        "if not posts_data_to_process:\n",
        "    print(\"No Posts data loaded or found for export.\")\n",
        "else:\n",
        "    print(f\"Found {total_posts_in_json} post entries in JSON. Preparing to export metadata for each media item individually...\")\n",
        "\n",
        "    excel_data_rows = []\n",
        "    copied_files_for_zip = []\n",
        "\n",
        "    column_names = [\n",
        "        'title', 'fulltext_url', 'additional_files', 'keywords', 'abstract',\n",
        "        'author1_fname', 'author1_mname', 'author1_lname', 'author1_suffix',\n",
        "        'author1_email', 'author1_institution', 'author1_is_corporate',\n",
        "        'disciplines', 'instagram_username', 'document_type', 'post_id' # Added post_id\n",
        "    ]\n",
        "\n",
        "    for post_index, post_item in enumerate(posts_data_to_process):\n",
        "        media_list_in_post = post_item.get('media')\n",
        "        if not media_list_in_post or not isinstance(media_list_in_post, list) or len(media_list_in_post) == 0:\n",
        "            # print(f\"Debug: Post item {post_index} has no media or media is not a list. Content: {post_item}\")\n",
        "            skipped_posts_no_media_list += 1\n",
        "            continue\n",
        "\n",
        "        # Determine post caption and timestamp (consistent for all media items in this post)\n",
        "        # Overall post caption is in post_item['title'] for carousels.\n",
        "        # For single media posts, it's often in post_item['media'][0]['title'].\n",
        "        # Post timestamp is in post_item['creation_timestamp'] for carousels or single media posts.\n",
        "        # For single media, it can also be in post_item['media'][0]['creation_timestamp'].\n",
        "\n",
        "        post_level_caption = post_item.get('title', '')\n",
        "        post_level_timestamp = post_item.get('creation_timestamp')\n",
        "\n",
        "        # Fallback for single media posts where caption/timestamp might be in the media item itself\n",
        "        first_media_item_for_fallback = media_list_in_post[0] if media_list_in_post else {}\n",
        "        if not isinstance(first_media_item_for_fallback, dict): # Ensure it's a dict before .get()\n",
        "            first_media_item_for_fallback = {}\n",
        "\n",
        "\n",
        "        if not post_level_caption: # If no overall post caption, try first media's title\n",
        "            post_level_caption = first_media_item_for_fallback.get('title', '')\n",
        "        if post_level_timestamp is None:\n",
        "            post_level_timestamp = first_media_item_for_fallback.get('creation_timestamp')\n",
        "\n",
        "        if post_level_timestamp is None:\n",
        "            # print(f\"Debug: Post item {post_index} has no valid timestamp. Content: {post_item}\")\n",
        "            skipped_posts_no_timestamp += 1\n",
        "            continue\n",
        "\n",
        "        # Use this derived post_level_timestamp as the post_id\n",
        "        current_post_id = post_level_timestamp\n",
        "        date_obj = datetime.fromtimestamp(current_post_id)\n",
        "        date_str_for_filename_and_title = date_obj.strftime('%Y-%m-%d')\n",
        "        handle_for_filename = instagram_handle.replace(\"@\", \"\") if instagram_handle else \"unknown_user\"\n",
        "\n",
        "        # Post-level abstract and keywords\n",
        "        abstract_for_post = post_level_caption\n",
        "        keywords_for_post = extract_hashtags(abstract_for_post)\n",
        "\n",
        "        media_items_successfully_processed_for_this_post = 0\n",
        "\n",
        "        for media_index, media_item in enumerate(media_list_in_post):\n",
        "            if not isinstance(media_item, dict):\n",
        "                # print(f\"Debug: Media item {media_index} in post {post_index} is not a dictionary. Content: {media_item}\")\n",
        "                continue\n",
        "\n",
        "            original_media_uri = media_item.get('uri')\n",
        "            if not original_media_uri:\n",
        "                # print(f\"Debug: Media item {media_index} in post {post_index} has no URI. Content: {media_item}\")\n",
        "                skipped_media_items_no_uri += 1\n",
        "                continue\n",
        "\n",
        "            if original_media_uri.startswith(('http://', 'https://')):\n",
        "                # print(f\"Info: Media item {media_index} ('{original_media_uri}') in post {post_index} is an absolute URI. Skipping.\")\n",
        "                skipped_media_items_absolute_uri += 1\n",
        "                continue\n",
        "\n",
        "            media_path = os.path.join(MEDIA_DIR, original_media_uri)\n",
        "            if not os.path.exists(media_path):\n",
        "                # print(f\"Warning: Media file not found for URI '{original_media_uri}' in post {post_index}. Path: {media_path}. Skipping media item.\")\n",
        "                skipped_media_items_missing_local_file += 1\n",
        "                continue\n",
        "\n",
        "            _, ext = os.path.splitext(original_media_uri)\n",
        "            if not ext:\n",
        "                if 'video_metadata' in media_item.get('media_metadata', {}): ext = '.mp4'\n",
        "                # Add more specific inferences if needed, e.g. for image types based on other metadata\n",
        "                else:\n",
        "                    # print(f\"Warning: Media URI '{original_media_uri}' in post {post_index} has no extension and type couldn't be inferred. Skipping media item.\")\n",
        "                    skipped_media_items_no_extension += 1\n",
        "                    continue\n",
        "\n",
        "            original_file_basename = os.path.splitext(os.path.basename(original_media_uri))[0]\n",
        "            sanitized_original_basename = ''.join(c if c.isalnum() else '_' for c in original_file_basename).strip('_')\n",
        "            if not sanitized_original_basename: # Handle cases where basename becomes empty after sanitizing\n",
        "                 sanitized_original_basename = f\"media_{media_index + 1}\"\n",
        "\n",
        "            # Filename for this specific media item\n",
        "            media_export_filename = f\"instagram_{handle_for_filename}_post_{date_str_for_filename_and_title}_p{post_index + 1}_m{media_index + 1}_{sanitized_original_basename}{ext}\"\n",
        "            media_export_path = os.path.join(output_dir, media_export_filename)\n",
        "\n",
        "            try:\n",
        "                copy2(media_path, media_export_path)\n",
        "                copied_files_for_zip.append(media_export_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error copying media file {media_path} to {media_export_path} for post {post_index}, media {media_index}: {e}. Skipping this media item.\")\n",
        "                skipped_media_items_missing_local_file += 1\n",
        "                continue\n",
        "\n",
        "            # DC Title for this specific media item\n",
        "            dc_title_for_media_item = f\"Instagram Post by {instagram_handle} - {date_str_for_filename_and_title} (Post {post_index + 1} - Media {media_index + 1} of {len(media_list_in_post)})\"\n",
        "\n",
        "            row = {\n",
        "                'title': dc_title_for_media_item,\n",
        "                'fulltext_url': media_export_filename, # This media item is the primary\n",
        "                'additional_files': '', # No additional files for this individual media item's record\n",
        "                'keywords': keywords_for_post, # From overall post\n",
        "                'abstract': abstract_for_post, # From overall post\n",
        "                'author1_fname': '', 'author1_mname': '', 'author1_lname': '', 'author1_suffix': '',\n",
        "                'author1_email': '', 'author1_institution': '', 'author1_is_corporate': False,\n",
        "                'disciplines': '',\n",
        "                'instagram_username': instagram_handle,\n",
        "                'document_type': 'Instagram Post', # Kept as 'Instagram Post'\n",
        "                'post_id': current_post_id # Timestamp of the original post for grouping\n",
        "            }\n",
        "            excel_data_rows.append(row)\n",
        "            processed_media_items_excel_count += 1\n",
        "            media_items_successfully_processed_for_this_post +=1\n",
        "\n",
        "        if media_list_in_post and media_items_successfully_processed_for_this_post == 0:\n",
        "             print(f\"Info: Post {post_index} had {len(media_list_in_post)} media items, but none could be processed successfully (e.g. all web links or files missing).\")\n",
        "\n",
        "\n",
        "    # --- Summary ---\n",
        "    print(f\"\\n--- Processing Summary for Posts (Individual Media Items) ---\")\n",
        "    print(f\"Total post entries in JSON: {total_posts_in_json}\")\n",
        "    print(f\"Total media items successfully processed and included in Excel: {processed_media_items_excel_count}\")\n",
        "    print(f\"Posts skipped entirely (e.g., no media list, no timestamp): {skipped_posts_no_media_list + skipped_posts_no_timestamp}\")\n",
        "    print(f\"  - Due to no media list or empty media list: {skipped_posts_no_media_list}\")\n",
        "    print(f\"  - Due to no valid timestamp for the post: {skipped_posts_no_timestamp}\")\n",
        "    print(f\"Individual media items skipped (breakdown):\")\n",
        "    print(f\"  - No URI in JSON entry: {skipped_media_items_no_uri}\")\n",
        "    print(f\"  - Web link (absolute HTTP/S URI): {skipped_media_items_absolute_uri}\")\n",
        "    print(f\"  - Local file missing or copy error: {skipped_media_items_missing_local_file}\")\n",
        "    print(f\"  - No file extension and type not inferred: {skipped_media_items_no_extension}\")\n",
        "    print(f\"-----------------------------------------------------------\\n\")\n",
        "\n",
        "    # --- Create DataFrame and save to Excel ---\n",
        "    if excel_data_rows:\n",
        "        df = pd.DataFrame(excel_data_rows, columns=column_names)\n",
        "        try:\n",
        "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"Metadata for {len(excel_data_rows)} post media items written to Excel: {excel_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Excel file {excel_path}: {e}\")\n",
        "            print(\"Make sure 'openpyxl' library is installed (e.g., !pip install openpyxl).\")\n",
        "    else:\n",
        "        print(\"No post media item data processed to write to Excel.\")\n",
        "\n",
        "    # --- Write README file ---\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"\"\"Instagram Posts Export Package (Digital Commons Format - Individual Media Items)\n",
        "===============================================================================\n",
        "\n",
        "Handle: @{instagram_handle}\n",
        "Exported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "This script processed {total_posts_in_json} post entries from 'posts_1.json'.\n",
        "\n",
        "This package contains:\n",
        "- Exported media files (images or videos) from Instagram posts that were found locally.\n",
        "- An Excel file ({excel_filename}) with metadata. Each row in this Excel file\n",
        "  represents a single media item (image or video) from an Instagram post.\n",
        "- This README.txt file.\n",
        "\n",
        "Structure of Excel Data:\n",
        "------------------------\n",
        "- Each media item from an Instagram post (including items from carousels) gets its own row.\n",
        "- The `post_id` column contains the creation timestamp of the original Instagram post.\n",
        "  This ID can be used to group all media items that originated from the same post.\n",
        "- The `fulltext_url` column lists the unique filename of the specific media item for that row.\n",
        "- The `additional_files` column is intentionally left blank for each row in this format.\n",
        "- Metadata common to the original post (like caption/abstract and keywords) is duplicated\n",
        "  across all rows corresponding to media items from that same post.\n",
        "\n",
        "Metadata Fields in Excel ({excel_filename}):\n",
        "---------------------------------------------\n",
        "- title: A generated title, unique for each media item, indicating its position within the original post\n",
        "         (e.g., \"Instagram Post by [username] - [YYYY-MM-DD] (Post [index] - Media [media_index] of [total_media])\").\n",
        "- fulltext_url: The filename of the exported media file for this specific row.\n",
        "- additional_files: Blank in this version.\n",
        "- keywords: Hashtags extracted from the original post's caption (repeated for all media from the same post).\n",
        "- abstract: The original caption/text of the Instagram post (repeated for all media from the same post).\n",
        "- author1_...: Author fields (defaulted to blank/False).\n",
        "- disciplines: Academic disciplines (defaulted to blank).\n",
        "- instagram_username: The Instagram handle.\n",
        "- document_type: \"Instagram Post\".\n",
        "- post_id: The creation timestamp of the original Instagram post, used for grouping.\n",
        "\n",
        "Processing Summary (from this export run):\n",
        "------------------------------------------\n",
        "- Total post entries in 'posts_1.json': {total_posts_in_json}\n",
        "- Total media items successfully processed and included in Excel: {processed_media_items_excel_count}\n",
        "- Posts skipped entirely (no media list or no timestamp): {skipped_posts_no_media_list + skipped_posts_no_timestamp}\n",
        "  - Due to no media list: {skipped_posts_no_media_list}\n",
        "  - Due to no timestamp: {skipped_posts_no_timestamp}\n",
        "- Individual media items skipped (breakdown):\n",
        "  - No URI in JSON: {skipped_media_items_no_uri}\n",
        "  - Web link (HTTP/S): {skipped_media_items_absolute_uri}\n",
        "  - Local file missing/error: {skipped_media_items_missing_local_file}\n",
        "  - No extension/type unknown: {skipped_media_items_no_extension}\n",
        "\n",
        "Generated by Instagram archive processing script.\n",
        "\"\"\")\n",
        "    print(f\"README file written to: {readme_path}\")\n",
        "\n",
        "    # --- Zip everything up ---\n",
        "    if copied_files_for_zip or (os.path.exists(excel_path) and excel_data_rows):\n",
        "        print(f\"Creating ZIP archive: {zip_path}\")\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            if os.path.exists(excel_path) and excel_data_rows:\n",
        "                zipf.write(excel_path, arcname=os.path.basename(excel_path))\n",
        "                print(f\"Added {os.path.basename(excel_path)} to ZIP.\")\n",
        "            elif not excel_data_rows: print(f\"Excel file {excel_filename} not added (no data).\")\n",
        "            elif not os.path.exists(excel_path): print(f\"Excel file {excel_path} not found, not added.\")\n",
        "\n",
        "            if os.path.exists(readme_path):\n",
        "                zipf.write(readme_path, arcname='README.txt')\n",
        "                print(f\"Added README.txt to ZIP.\")\n",
        "            else: print(f\"README.txt not found at {readme_path}, not added.\")\n",
        "\n",
        "            successfully_zipped_media_count = 0\n",
        "            for file_to_zip in copied_files_for_zip:\n",
        "                if os.path.exists(file_to_zip):\n",
        "                    zipf.write(file_to_zip, arcname=os.path.basename(file_to_zip))\n",
        "                    successfully_zipped_media_count += 1\n",
        "                else:\n",
        "                    print(f\"Warning: Copied media file {file_to_zip} not found during zipping, not added.\")\n",
        "            print(f\"Added {successfully_zipped_media_count} media files to ZIP.\")\n",
        "        print(f\"ZIP archive '{zip_package_filename}' created successfully in '{output_dir}'.\")\n",
        "    else:\n",
        "        print(f\"No files processed/copied for posts. ZIP archive '{zip_package_filename}' not created.\")\n",
        "\n",
        "print(f\"\\n--- Post Export Process (Individual Media Items) Finished ---\")\n",
        "\n",
        "# To download this specific package, the next cell would need:\n",
        "# source_zip_path = \"/content/extracted_data/posts_export_dc_format_individual_media/posts_package_dc_format_individual_media.zip\"\n",
        "# target_zip_filename = \"posts_individual_media.zip\" # Or similar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVMyWE2Pm7PH",
        "outputId": "590b0666-ddac-44aa-c363-fc6cbd847bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directory for posts (individual media items): /content/extracted_data/posts_export_dc_format_individual_media\n",
            "Using Instagram handle: umsllibraries\n",
            "Found 646 post entries in JSON. Preparing to export metadata for each media item individually...\n",
            "\n",
            "--- Processing Summary for Posts (Individual Media Items) ---\n",
            "Total post entries in JSON: 646\n",
            "Total media items successfully processed and included in Excel: 860\n",
            "Posts skipped entirely (e.g., no media list, no timestamp): 0\n",
            "  - Due to no media list or empty media list: 0\n",
            "  - Due to no valid timestamp for the post: 0\n",
            "Individual media items skipped (breakdown):\n",
            "  - No URI in JSON entry: 0\n",
            "  - Web link (absolute HTTP/S URI): 0\n",
            "  - Local file missing or copy error: 0\n",
            "  - No file extension and type not inferred: 0\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Metadata for 860 post media items written to Excel: /content/extracted_data/posts_export_dc_format_individual_media/posts_metadata_dc_format_individual_media.xlsx\n",
            "README file written to: /content/extracted_data/posts_export_dc_format_individual_media/README.txt\n",
            "Creating ZIP archive: /content/extracted_data/posts_export_dc_format_individual_media/posts_package_dc_format_individual_media.zip\n",
            "Added posts_metadata_dc_format_individual_media.xlsx to ZIP.\n",
            "Added README.txt to ZIP.\n",
            "Added 860 media files to ZIP.\n",
            "ZIP archive 'posts_package_dc_format_individual_media.zip' created successfully in '/content/extracted_data/posts_export_dc_format_individual_media'.\n",
            "\n",
            "--- Post Export Process (Individual Media Items) Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Assuming your previous script has run and created the source ZIP ---\n",
        "# Define the source and target paths\n",
        "source_zip_path = \"/content/extracted_data/posts_export_dc_format_individual_media/posts_package_dc_format_individual_media.zip\"\n",
        "target_base_dir = \"/content/batchup\" # The directory where you want the new ZIP\n",
        "target_zip_filename = \"posts.zip\"     # The desired name for the ZIP in the target directory\n",
        "target_zip_path = os.path.join(target_base_dir, target_zip_filename)\n",
        "\n",
        "# 1. Check if the source ZIP file exists\n",
        "if not os.path.exists(source_zip_path):\n",
        "    print(f\"‚ùå ERROR: Source ZIP file not found at: {source_zip_path}\")\n",
        "    print(\"Please ensure the previous steps to create the ZIP were successful.\")\n",
        "else:\n",
        "    print(f\"‚úîÔ∏è Source ZIP found: {source_zip_path}\")\n",
        "\n",
        "    # 2. Ensure the target directory exists, create it if not\n",
        "    os.makedirs(target_base_dir, exist_ok=True)\n",
        "    print(f\"‚úîÔ∏è Ensured target directory exists: {target_base_dir}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Copy the file\n",
        "        shutil.copy2(source_zip_path, target_zip_path) # copy2 preserves metadata\n",
        "        print(f\"‚úÖ Successfully copied '{os.path.basename(source_zip_path)}' to '{target_zip_path}'\")\n",
        "\n",
        "        # 4. Offer a download link for the new file\n",
        "        print(f\"\\n‚¨áÔ∏è Click the link below to download '{target_zip_filename}':\")\n",
        "        # Note: files.download() directly initiates the download in the browser\n",
        "        # It doesn't print a clickable link in the classic HTML sense in the output cell,\n",
        "        # but Colab's UI will typically show a download prompt or progress.\n",
        "        files.download(target_zip_path)\n",
        "        print(f\"(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\")\n",
        "        print(f\"The file is located at: {target_zip_path} in the Colab environment.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during copy or download: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Guq-sGYZnFSy",
        "outputId": "f1d44379-f7f7-4083-80ca-9f0d4a7ca8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Source ZIP found: /content/extracted_data/posts_export_dc_format_individual_media/posts_package_dc_format_individual_media.zip\n",
            "‚úîÔ∏è Ensured target directory exists: /content/batchup\n",
            "‚úÖ Successfully copied 'posts_package_dc_format_individual_media.zip' to '/content/batchup/posts.zip'\n",
            "\n",
            "‚¨áÔ∏è Click the link below to download 'posts.zip':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71d27d64-0d65-43af-8263-54168ee39593\", \"posts.zip\", 150039356)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(If download doesn't start automatically, check your browser's download manager or pop-up blocker.)\n",
            "The file is located at: /content/batchup/posts.zip in the Colab environment.\n"
          ]
        }
      ]
    }
  ]
}
